## LangChain 실전 가이드 [14편]: 내 LLM 애플리케이션, 잘 작동하고 있을까? LangChain 평가(Evaluation) 시작하기

안녕하세요! LangChain 블로그 시리즈 독자 여러분. 지금까지 우리는 LangChain의 기본 개념부터 RAG, Agent, 그리고 LangSmith와 LangServe를 이용한 디버깅 및 배포까지 숨 가쁘게 달려왔습니다. 이제 여러분은 직접 LLM 애플리케이션을 구축할 수 있는 능력을 갖추셨을 겁니다.

하지만 여기서 중요한 질문이 생깁니다. **"내가 만든 LLM 애플리케이션이 과연 잘 작동하고 있을까?"**, **"기대하는 만큼의 성능을 내고 있을까?"** 단순히 '오류 없이 실행된다'는 것을 넘어, 애플리케이션의 품질과 신뢰성을 객관적으로 측정하는 것은 매우 중요합니다.

이번 14편에서는 바로 이 **LLM 애플리케이션 평가(Evaluation)**의 중요성과 LangChain이 제공하는 평가 기능에 대해 알아보겠습니다. LangSmith를 활용하여 평가 과정을 더욱 효율적으로 만드는 방법까지 함께 살펴볼 것입니다.

**🎯 이번 글의 목표:**

1.  LLM 애플리케이션 평가의 중요성과 어려움을 이해합니다.
2.  LangChain의 평가(Evaluation) 모듈 기본 개념을 학습합니다.
3.  다양한 종류의 평가자(Evaluator) 사용법을 익힙니다.
4.  LangSmith를 활용하여 평가 데이터셋을 구축하고 평가를 실행하는 방법을 알아봅니다.

---

###  🤔 LLM 애플리케이션 평가, 왜 중요하고 어려울까?

기존 소프트웨어 테스팅과 달리 LLM 애플리케이션 평가는 다음과 같은 독특한 어려움을 가집니다.

1.  **정답의 부재 (Lack of Ground Truth):** 요약, 창의적 글쓰기 등 많은 LLM 작업에는 명확한 '정답'이 없습니다. 무엇이 '좋은' 결과인지 정의하기 어렵습니다.
2.  **주관성 (Subjectivity):** 결과물의 품질 평가는 주관적일 수 있습니다. '유용함', '흥미로움', '적절한 톤'과 같은 기준은 사람마다 다르게 해석될 수 있습니다.
3.  **일관성 부족 (Inconsistency):** 같은 입력에 대해서도 LLM은 다른 결과물을 생성할 수 있습니다. (Temperature 설정 등에 따라)
4.  **복잡성 (Complexity):** RAG, Agent와 같이 여러 컴포넌트가 얽힌 애플리케이션은 특정 실패 지점을 찾아내기 어렵습니다. 검색된 문서가 문제인지, 프롬프트가 문제인지, LLM 자체의 응답이 문제인지 파악하기 힘듭니다.
5.  **비용 및 시간 (Cost & Time):** 대규모 데이터셋에 대해 일일이 사람이 평가하는 것은 시간과 비용이 많이 듭니다.

이러한 어려움에도 불구하고 평가는 **반드시** 필요합니다. 평가는 다음과 같은 이점을 제공합니다.

*   **품질 보증 (Quality Assurance):** 애플리케이션이 일관되고 신뢰성 있는 결과를 제공하는지 확인합니다.
*   **회귀 테스트 (Regression Testing):** 코드 변경, 프롬프트 수정, 모델 업데이트 후 성능 저하가 발생하지 않았는지 검증합니다.
*   **모델/프롬프트 비교 (Model/Prompt Comparison):** 어떤 LLM 모델, 프롬프트 템플릿, 또는 RAG 설정이 더 나은 성능을 보이는지 객관적으로 비교하여 최적의 조합을 찾습니다.
*   **개선 방향 제시 (Identifying Areas for Improvement):** 평가 결과를 분석하여 애플리케이션의 약점을 파악하고 개선 방향을 설정할 수 있습니다.
*   **사용자 만족도 향상 (Improving User Satisfaction):** 꾸준한 평가와 개선을 통해 사용자에게 더 높은 가치를 제공하고 신뢰를 구축합니다.

---

### ⚙️ LangChain 평가(Evaluation) 모듈 소개

LangChain은 이러한 LLM 애플리케이션 평가의 어려움을 해소하기 위해 자체적인 **평가(Evaluation) 모듈**을 제공합니다. LangChain 평가 모듈의 핵심 아이디어는 **평가 자체에 LLM을 활용**하거나, **미리 정의된 기준(Criteria) 또는 함수**를 사용하여 애플리케이션의 출력을 평가하는 것입니다.

주요 구성 요소는 다음과 같습니다.

1.  **평가자 (Evaluators):** 특정 기준에 따라 입력(input)과 출력(output), 그리고 때로는 참조(reference) 레이블을 비교하여 점수(score)나 평가 결과(feedback)를 생성하는 객체입니다. 다양한 종류의 평가자가 미리 구현되어 있습니다.
2.  **데이터셋 (Datasets):** 평가를 수행하기 위한 입력, 출력 예시 및 정답(선택 사항)의 모음입니다. LangSmith를 사용하면 이러한 데이터셋을 쉽게 생성하고 관리할 수 있습니다.
3.  **(선택) 평가 체인 (`RunEvalChain` 등):** 특정 데이터셋에 대해 평가자를 실행하고 결과를 집계하는 데 사용될 수 있는 유틸리티 체인입니다.

---

### 🚀 다양한 평가자(Evaluator) 활용법

LangChain은 다양한 시나리오에 맞는 평가자들을 제공합니다. 몇 가지 주요 평가자와 사용 예시를 살펴보겠습니다.

**(사전 준비)** 평가를 위해서는 LLM이 필요합니다. (예: OpenAI 모델)

```python
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# OpenAI API 키 설정 (환경 변수 사용 권장)
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"

# 평가에 사용할 LLM (Criteria Evaluator 등에서 사용)
llm = ChatOpenAI(model="gpt-4", temperature=0)

# 예시: 간단한 요약 Chain
summarize_prompt = ChatPromptTemplate.from_template("다음 텍스트를 한 문장으로 요약해줘: {text}")
summarizer_chain = summarize_prompt | llm | StrOutputParser()

# 평가할 예시 텍스트 및 예측 결과
example_text = "LangChain은 LLM을 활용한 애플리케이션 개발을 돕는 프레임워크입니다. 다양한 컴포넌트와 체인을 제공하여 복잡한 워크플로우 구축을 간소화합니다."
prediction = summarizer_chain.invoke({"text": example_text})
print(f"예측 요약: {prediction}")

# 사람이 작성한 참조 요약 (선택 사항)
reference_summary = "LangChain은 LLM 앱 개발을 위한 프레임워크로, 컴포넌트와 체인을 통해 복잡한 워크플로우를 단순화합니다."
```

#### 1. 문자열 비교 평가자 (String Evaluators)

가장 기본적인 평가 방식으로, 예측 결과 문자열과 참조 문자열을 직접 비교합니다.

*   **`ExactMatchStringEvaluator`**: 예측과 참조가 정확히 일치하는지 확인합니다.

    ```python
    from langchain.evaluation import ExactMatchStringEvaluator

    evaluator = ExactMatchStringEvaluator()
    result = evaluator.evaluate_strings(
        prediction=prediction,
        reference=reference_summary,
        input=example_text # 평가지표 계산에 input이 필요할 수 있음 (선택적)
    )
    print(f"정확 일치 평가 결과: {result}")
    # {'score': 0} # 정확히 같지 않으므로 0
    ```

*   **`StringDistanceEvaluator`**: 문자열 간의 편집 거리(Levenshtein 등)를 계산하여 유사도를 평가합니다.

    ```python
    from langchain.evaluation import StringDistanceEvaluator

    evaluator = StringDistanceEvaluator() # 기본적으로 Levenshtein 거리 사용
    result = evaluator.evaluate_strings(
        prediction=prediction,
        reference=reference_summary
    )
    print(f"문자열 거리 기반 평가 결과 (낮을수록 유사): {result}")
    # {'score': 0.8...} # 유사도 점수 (정확한 값은 예측 결과에 따라 다름)
    ```

*   **`EmbeddingDistanceEvaluator`**: 임베딩 벡터 간의 거리를 측정하여 의미론적 유사도를 평가합니다. 철자나 단어 순서가 달라도 의미가 비슷하면 높은 점수를 받을 수 있습니다.

    ```python
    from langchain.evaluation import EmbeddingDistanceEvaluator
    from langchain_openai import OpenAIEmbeddings

    # 임베딩 모델 준비
    embeddings = OpenAIEmbeddings()

    evaluator = EmbeddingDistanceEvaluator(embeddings=embeddings) # 기본적으로 코사인 유사도 사용
    result = evaluator.evaluate_strings(
        prediction=prediction,
        reference=reference_summary
    )
    print(f"임베딩 거리 기반 평가 결과 (높을수록 유사): {result}")
    # {'score': 0.9...} # 의미적 유사도 점수 (정확한 값은 예측/참조에 따라 다름)
    ```

#### 2. 기준 기반 평가자 (Criteria Evaluators)

미리 정의된 기준(예: 간결성, 관련성, 유해성 없음)에 따라 LLM 자체가 예측 결과를 평가하도록 합니다. 참조 레이블이 없어도 평가가 가능합니다.

*   **`CriteriaEvalChain`**: 가장 일반적인 기준 기반 평가자입니다.

    ```python
    from langchain.evaluation import load_evaluator

    # "간결성(conciseness)" 기준 평가자 로드
    evaluator = load_evaluator("criteria", criteria="conciseness", llm=llm)

    result = evaluator.evaluate_strings(
        prediction=prediction,
        input=example_text
    )
    print(f"간결성 평가 결과: {result}")
    # {'score': 1, 'reasoning': 'The summary is concise and captures the main point of the original text in a single sentence.'}

    # 여러 기준을 동시에 평가할 수도 있습니다.
    custom_criteria = {
        "accuracy": "예측이 원본 텍스트의 핵심 정보를 정확하게 반영하는가?",
        "clarity": "예측이 명확하고 이해하기 쉬운가?"
    }
    evaluator = load_evaluator("criteria", criteria=custom_criteria, llm=llm)
    result = evaluator.evaluate_strings(
        prediction=prediction,
        input=example_text
    )
    print(f"사용자 정의 기준 평가 결과: {result}")
    # {'accuracy': 'Y', 'clarity': 'Y', 'reasoning': ...}
    ```

#### 3. 레이블 기반 평가자 (Labeled Evaluators)

참조 레이블(정답)이 있는 경우, 예측 결과와 비교하여 평가합니다. (위 String Evaluator 예시도 레이블 기반의 일종입니다.)

*   **`LabeledCriteriaEvalChain`**: 기준 기반 평가에 참조 레이블을 추가로 활용하여 더 정확한 평가를 시도합니다.

    ```python
    evaluator = load_evaluator("labeled_criteria", criteria="correctness", llm=llm)

    result = evaluator.evaluate_strings(
        prediction=prediction,
        reference=reference_summary,
        input=example_text
    )
    print(f"레이블 기반 정확성 평가 결과: {result}")
    # {'score': 1, 'reasoning': ...}
    ```

#### 4. 쌍 비교 평가자 (Pairwise Evaluators)

두 개의 다른 예측 결과(예: 다른 모델 또는 프롬프트로부터 생성된 결과)를 LLM이 비교하여 어느 쪽이 더 나은지 평가합니다. A/B 테스트에 유용합니다.

*   **`PairwiseStringEvalChain`**: 두 예측 결과를 LLM이 비교 평가합니다.

    ```python
    # 다른 모델(예: gpt-3.5-turbo)로 예측 생성 가정
    llm_gpt35 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    chain_gpt35 = summarize_prompt | llm_gpt35 | StrOutputParser()
    prediction_gpt35 = chain_gpt35.invoke({"text": example_text})
    print(f"GPT-3.5 예측: {prediction_gpt35}")

    evaluator = load_evaluator("pairwise_string", llm=llm)

    result = evaluator.evaluate_string_pairs(
        prediction=prediction, # GPT-4 결과
        prediction_b=prediction_gpt35, # GPT-3.5 결과
        input=example_text
    )
    print(f"쌍 비교 평가 결과 (어느 쪽이 더 나은가?): {result}")
    # {'score': 1, 'reasoning': ...} # score=1이면 prediction(A) 선호, 0이면 prediction_b(B) 선호
    ```

---

### <0xF0><0x9F><0xA7><0xAA> LangSmith를 활용한 평가 워크플로우

LangChain 평가 모듈은 LangSmith와 함께 사용할 때 더욱 강력해집니다. LangSmith는 평가 데이터셋을 구축하고, 여러 평가자를 동시에 실행하며, 그 결과를 시각화하고 분석하는 통합 환경을 제공합니다.

1.  **데이터셋 생성 (Dataset Creation):**
    *   LangSmith UI에서 직접 예제(입력, 참조 출력 등)를 추가하여 데이터셋을 만들 수 있습니다.
    *   애플리케이션 실행 로그(Trace) 중에서 중요한 예시를 선택하여 데이터셋으로 보낼 수 있습니다. (예: 특정 사용자의 질문과 챗봇의 답변, RAG 결과 등)
    *   CSV 파일 등을 업로드하여 데이터셋을 구성할 수도 있습니다.

2.  **평가 실행 (Running Evaluations):**
    *   LangSmith UI의 'Datasets & Testing' 탭에서 생성된 데이터셋을 선택합니다.
    *   해당 데이터셋의 각 예제에 대해 평가를 실행할 LangChain 애플리케이션(Chain, Agent 등)을 지정합니다. (애플리케이션을 실행하여 '예측' 결과를 생성하기 위함)
    *   적용할 평가자(Evaluator)를 하나 이상 선택합니다. LangSmith는 LangChain의 다양한 내장 평가자를 지원하며, 직접 정의한 평가자도 사용할 수 있습니다.
    *   'Run Evaluation' 버튼을 클릭하면, LangSmith가 백그라운드에서 데이터셋의 모든 예제에 대해 애플리케이션을 실행하고, 선택된 평가자들을 이용해 결과를 평가합니다.

3.  **결과 분석 (Analyzing Results):**
    *   평가가 완료되면 LangSmith는 각 예제별 평가 점수와 피드백(LLM 기반 평가의 경우 'reasoning')을 테이블 형태로 보여줍니다.
    *   평가 점수를 기준으로 정렬하거나 필터링하여 성능이 낮은 예제(실패 사례)를 쉽게 식별할 수 있습니다.
    *   각 예제의 상세 실행 과정(Trace)과 평가 결과를 함께 보며 문제의 원인을 분석할 수 있습니다. (예: RAG에서 잘못된 문서를 검색했는지, 프롬프트가 오해의 소지가 있었는지 등)
    *   시간 경과에 따른 평가 점수 변화를 추적하여 애플리케이션 개선 효과나 성능 저하 여부를 모니터링할 수 있습니다.

**LangSmith를 활용하면, 코드 몇 줄 없이도 GUI를 통해 체계적인 평가 프로세스를 구축하고 관리할 수 있어 생산성이 크게 향상됩니다.**

---

### ✨ 효과적인 평가를 위한 팁

*   **작게 시작하세요:** 처음부터 모든 것을 평가하려 하지 말고, 애플리케이션의 가장 중요한 기능이나 핵심적인 실패 모드에 집중하여 간단한 평가부터 시작하세요.
*   **다양한 평가 방법 조합:** LLM 기반 평가(Criteria, Pairwise)와 정량적 평가(String Distance, Embedding Distance), 그리고 필요한 경우 사람의 주관적 평가를 조합하여 다각적으로 분석하는 것이 좋습니다.
*   **대표적인 데이터셋 구축:** 실제 사용 사례를 반영하는 다양한 데이터(성공 사례, 실패 사례, 엣지 케이스 등)를 포함하여 평가 데이터셋을 구축해야 신뢰성 있는 결과를 얻을 수 있습니다.
*   **지속적인 평가와 개선:** 평가는 일회성 이벤트가 아닙니다. 애플리케이션을 수정하거나 개선할 때마다 평가를 다시 수행하여 변경 사항의 영향을 확인하고, 꾸준히 품질을 관리하는 **반복적인 프로세스**로 만들어야 합니다. LangSmith는 이러한 반복 과정을 효율적으로 관리하는 데 큰 도움을 줍니다.

---

### 🏁 마무리

이번 시간에는 LangChain 애플리케이션의 품질을 측정하고 개선하기 위한 필수 과정인 **평가(Evaluation)**에 대해 알아보았습니다. 평가의 중요성과 어려움, LangChain 평가 모듈의 다양한 평가자 종류와 사용법, 그리고 LangSmith를 활용한 효율적인 평가 워크플로우까지 살펴보았습니다.

단순히 LLM 애플리케이션을 만드는 것을 넘어, 그것이 **얼마나 잘 작동하는지 객관적으로 파악하고 지속적으로 개선**해 나가는 것이야말로 성공적인 LLM 애플리케이션 개발의 핵심입니다. LangChain의 평가 기능과 LangSmith를 적극적으로 활용하여 여러분의 애플리케이션을 더욱 강력하고 신뢰성 있게 만들어 보시길 바랍니다.

다음, 마지막 15편에서는 LangChain 생태계의 다른 구성 요소들과 앞으로의 발전 방향을 조망하며 시리즈를 마무리하도록 하겠습니다.

궁금한 점이나 의견이 있으시면 언제든지 댓글로 남겨주세요!