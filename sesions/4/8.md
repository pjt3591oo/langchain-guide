## LangChain 블로그 시리즈 [8편]: 챗봇에 기억력을! LangChain Memory의 종류와 활용법

안녕하세요! LangChain 시리즈 8편입니다. 지난 7편에서는 Retriever와 LCEL을 활용하여 우리만의 문서에 기반해 질문에 답변하는 완전한 RAG(Retrieval-Augmented Generation) Chain을 성공적으로 구축했습니다. 하지만 이 RAG Chain에는 한 가지 아쉬운 점이 있습니다. 바로 **'기억력'이 없다**는 것입니다.

이전 대화에서 어떤 질문을 했고 어떤 답변을 받았는지 전혀 기억하지 못하기 때문에, 매번 질문을 독립적인 것으로 간주합니다. 예를 들어, "LangChain을 개발한 회사는 어디인가요?"라고 묻고 "Harrison Chase가 창립했습니다."라는 답변을 받은 후, "그가 또 무엇을 만들었나요?"라고 물으면 우리의 RAG 챗봇은 '그(he)'가 누구인지 알지 못해 제대로 답변할 수 없습니다.

이처럼 자연스러운 대화 흐름을 만들고 사용자와의 상호작용 맥락을 유지하기 위해 챗봇에게는 **기억(Memory)** 이 필수적입니다. 이번 시간에는 LangChain의 **Memory** 컴포넌트를 사용하여 어떻게 챗봇에 기억력을 부여하는지, 다양한 Memory 종류는 무엇이 있는지, 그리고 이를 LCEL Chain에 통합하는 방법(특히 `RunnableWithMessageHistory`)에 대해 알아보겠습니다.

### 왜 챗봇은 기억이 필요할까요?

앞서 예시에서 보았듯이, 기억이 없는 챗봇과의 대화는 매우 부자연스럽고 단절적입니다. 사용자는 매번 완전한 문맥을 제공해야 하며, 챗봇은 이전 대화 내용을 활용하지 못합니다. Memory는 다음과 같은 이유로 중요합니다.

1.  **맥락 유지 (Context Preservation):** 대화의 흐름을 파악하고 이전 발언을 참조하여 답변을 생성합니다. ("방금 말한 그것에 대해 더 자세히 알려줘.")
2.  **자연스러운 대화 (Natural Conversation):** 사용자가 반복해서 정보를 제공할 필요 없이 사람과 대화하는 듯한 경험을 제공합니다.
3.  **개인화 (Personalization):** 사용자의 이름, 선호도 등 이전 대화에서 언급된 정보를 기억하여 맞춤형 응답을 제공할 수 있습니다. (물론 개인정보 보호에 유의해야 합니다.)

### LangChain Memory: 작동 원리

LangChain의 Memory 컴포넌트는 Chain이나 Agent가 실행되는 동안 상태(주로 대화 기록)를 유지하는 역할을 합니다. 기본적인 작동 방식은 다음과 같습니다.

1.  **기록 로드 (Load History):** Chain 실행 전에, Memory는 이전 대화 기록을 불러옵니다.
2.  **프롬프트 주입 (Inject into Prompt):** 불러온 대화 기록을 프롬프트 템플릿의 지정된 변수(예: `{history}`)에 삽입합니다.
3.  **LLM 호출 (Call LLM):** 현재 입력과 이전 대화 기록이 포함된 프롬프트를 LLM에 전달하여 응답을 생성합니다.
4.  **기록 업데이트 (Update History):** Chain 실행 후, Memory는 방금 발생한 사용자 입력과 LLM의 응답을 대화 기록에 추가하여 저장합니다.

이 과정을 통해 Chain은 이전 대화 내용을 '기억'하고 이를 다음 응답 생성에 활용할 수 있습니다.

### 다양한 LangChain Memory 종류 살펴보기

LangChain은 대화 기록을 저장하고 관리하는 방식에 따라 다양한 종류의 Memory를 제공합니다. 필요에 따라 적절한 것을 선택해야 합니다.

1.  **`ConversationBufferMemory`**:
    * **작동 방식:** 전체 대화 기록(사용자 입력, AI 응답)을 있는 그대로 버퍼에 저장합니다.
    * **장점:** 간단하고, 짧은 대화에서는 전체 맥락을 완벽하게 유지합니다.
    * **단점:** 대화가 길어지면 기록이 너무 커져 LLM의 컨텍스트 창 제한을 초과하거나 API 비용/처리 시간이 증가할 수 있습니다.
    * **사용 코드 (개념):**
        ```python
        from langchain.memory import ConversationBufferMemory
        memory = ConversationBufferMemory()
        memory.save_context({"input": "안녕하세요!"}, {"output": "반갑습니다! 무엇을 도와드릴까요?"})
        print(memory.load_memory_variables({}))
        # 출력: {'history': 'Human: 안녕하세요!\nAI: 반갑습니다! 무엇을 도와드릴까요?'}
        ```

2.  **`ConversationBufferWindowMemory`**:
    * **작동 방식:** `ConversationBufferMemory`와 유사하지만, 가장 최근의 `k`개 대화 턴만 저장합니다.
    * **장점:** 대화 기록의 크기를 제한하여 컨텍스트 창 문제를 방지합니다.
    * **단점:** `k`개 이전의 대화 내용은 소실됩니다.
    * **사용 코드 (개념):**
        ```python
        from langchain.memory import ConversationBufferWindowMemory
        memory = ConversationBufferWindowMemory(k=3) # 최근 3개 대화 턴 저장
        # ... save_context, load_memory_variables 사용은 유사 ...
        ```

3.  **`ConversationSummaryMemory`**:
    * **작동 방식:** 대화가 진행됨에 따라 LLM을 사용하여 이전 대화 내용을 요약하고, 이 요약본을 저장합니다.
    * **장점:** 매우 긴 대화에서도 핵심 내용을 간결하게 유지하여 컨텍스트 제한 문제를 해결할 수 있습니다.
    * **단점:** 요약 과정에서 LLM 호출이 추가로 발생하여 비용과 지연 시간이 증가하며, 요약 과정에서 일부 정보가 손실될 수 있습니다. 요약을 위한 LLM 객체가 필요합니다.
    * **사용 코드 (개념):**
        ```python
        from langchain.memory import ConversationSummaryMemory
        from langchain_openai import ChatOpenAI
        llm_for_summary = ChatOpenAI(temperature=0)
        memory = ConversationSummaryMemory(llm=llm_for_summary)
        # ... save_context, load_memory_variables 사용은 유사 ... (load 시 요약본 반환)
        ```

4.  **`ConversationSummaryBufferMemory`**:
    * **작동 방식:** 최근 대화는 버퍼에 그대로 저장하고(`ConversationBufferMemory`), 오래된 대화는 요약하여(`ConversationSummaryMemory`) 관리합니다. 버퍼 크기가 특정 길이(`max_token_limit`)를 초과하면 가장 오래된 대화를 요약하여 요약본에 추가합니다.
    * **장점:** 최근 대화의 상세함과 긴 대화의 요약 관리 능력을 결합한 균형 잡힌 방식입니다.
    * **단점:** 구현이 다소 복잡하고 요약 비용이 발생합니다.
    * **사용 코드 (개념):**
        ```python
        from langchain.memory import ConversationSummaryBufferMemory
        # ... llm, max_token_limit 설정 필요 ...
        ```

이 외에도 대화에서 중요한 개체(Entity)와 그 관계를 추출하여 지식 그래프(Knowledge Graph) 형태로 저장하는 `ConversationKGMemory` 등 특수한 목적의 Memory도 있습니다.

### LCEL과 Memory 통합: `RunnableWithMessageHistory` 사용하기

LCEL(LangChain Expression Language)로 구축된 Chain은 기본적으로 상태가 없습니다(stateless). 즉, 각 실행이 독립적이며 이전 실행 결과를 기억하지 못합니다. Memory를 통합하려면 대화 기록을 로드하고 업데이트하는 상태 관리 메커니즘이 필요합니다.

LangChain은 이를 위해 **`RunnableWithMessageHistory`** 라는 강력한 도구를 제공합니다. 이 클래스는 기존의 LCEL Runnable 객체(우리가 7편에서 만든 RAG Chain 등)를 감싸서, 자동으로 대화 기록을 관리하고 Chain 실행 시 주입해주는 역할을 합니다.

`RunnableWithMessageHistory`를 사용하려면 다음 요소들이 필요합니다.

1.  **핵심 Runnable:** 상태 관리를 추가할 기존 LCEL Chain (예: 우리의 RAG Chain).
2.  **세션 기록(Session History) 관리 함수:** 각 대화 세션(사용자별 또는 대화별)의 기록을 가져오는 방법을 정의한 함수. LangChain은 `ChatMessageHistory`라는 기본 클래스와 이를 구현한 `InMemoryChatMessageHistory` (메모리에 저장), `FileChatMessageHistory` (파일에 저장) 등을 제공합니다.
3.  **입력/출력/기록 메시지 키:** Chain의 입력, 출력, 그리고 주입될 대화 기록에 해당하는 키 이름을 지정합니다.

**단계별 통합 과정:**

1.  **프롬프트 수정:** Memory에서 가져온 대화 기록을 받을 변수(보통 `{history}`)를 프롬프트 템플릿에 추가합니다.
2.  **세션 기록 저장소 준비:** 각 대화 세션 ID별로 `ChatMessageHistory` 객체를 저장하고 관리할 방법을 마련합니다 (간단하게는 파이썬 딕셔너리 사용).
3.  **`RunnableWithMessageHistory`로 Chain 감싸기:** 기존 Chain, 세션 기록 관리 함수, 메시지 키들을 인자로 전달하여 `RunnableWithMessageHistory` 객체를 생성합니다.

### RAG 챗봇에 기억력 추가하기! (예제 코드)

이제 7편에서 만든 RAG Chain에 `ConversationBufferWindowMemory`의 기능을 `RunnableWithMessageHistory`를 통해 통합해 보겠습니다.

```python
import os
from operator import itemgetter

from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory # InMemoryChatMessageHistory 사용

# --- 환경 변수 및 이전 설정 로드 (가정) ---
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"
persist_directory = "./chroma_db_example"
embeddings_model = HuggingFaceEmbeddings(...) # 6편과 동일하게 로드
vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings_model)
retriever = vector_store.as_retriever(search_kwargs={'k': 3})
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
output_parser = StrOutputParser()

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
# -------------------------------------------

# 1. 프롬프트 수정: MessagesPlaceholder 추가
# history 변수를 사용하여 이전 대화 기록을 주입할 위치 지정
contextualize_q_system_prompt = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history. Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is."""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)
# 질문 재구성 chain (컨텍스트 반영)
contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()

qa_system_prompt = """You are an assistant for question-answering tasks. \
Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. \
Use three sentences maximum and keep the answer concise.\

{context}"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder(variable_name="history"), # 여기에도 history 추가
        ("human", "{question}"),
    ]
)

# RAG Chain의 핵심 로직 (질문 -> 문서 검색 및 답변 생성)
def contextualized_question(input: dict):
    if input.get("history"):
        return contextualize_q_chain
    else:
        return input["question"]

rag_chain_core = (
    RunnableParallel(
        {
            "context": itemgetter("question") | retriever | format_docs,
            "question": itemgetter("question"),
            "history": itemgetter("history") # history 전달 추가
        }
    )
    | qa_prompt
    | llm
    | output_parser # 최종 답변 생성
)

# 전체 RAG Chain (컨텍스트 질문 생성 포함)
# RunnableLambda를 사용하여 history 유무에 따라 분기
chain_with_context_handling = RunnableLambda(
    lambda x: contextualized_question(x)
) | rag_chain_core

# 2. 세션 기록 저장소 및 관리 함수 정의
store = {} # 세션 ID별 기록 저장을 위한 딕셔너리

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        # k=5: 최근 5개 대화 턴(질문+답변=1턴) 저장
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# 3. RunnableWithMessageHistory로 Chain 감싸기
conversational_rag_chain = RunnableWithMessageHistory(
    chain_with_context_handling, # 핵심 로직 체인
    get_session_history,         # 세션 기록 관리 함수
    input_messages_key="question", # 사용자의 현재 질문 키
    history_messages_key="history", # 프롬프트에 주입될 기록 키
    output_messages_key="answer"    # LLM의 최종 답변 키 (이 예제에서는 output_parser가 문자열 반환)
                                    # 실제로는 LLM의 ChatMessage 출력을 사용한다면 answer 대신 다른 키 지정 가능
)

print("기억력이 추가된 RAG Chain이 준비되었습니다!")

# --- 대화 실행 (동일한 session_id 사용) ---
session_id_1 = "user123"

print("\n--- 대화 시작 (세션 ID: user123) ---")

# 첫 번째 질문
question1 = "What is LangChain?"
print(f"\n[User] {question1}")
answer1 = conversational_rag_chain.invoke(
    {"question": question1},
    config={"configurable": {"session_id": session_id_1}}
)
print(f"[AI] {answer1}")

# 두 번째 질문 (이전 답변과 연관)
question2 = "Who created it?" # 'it'이 LangChain을 가리킴
print(f"\n[User] {question2}")
answer2 = conversational_rag_chain.invoke(
    {"question": question2},
    config={"configurable": {"session_id": session_id_1}}
)
print(f"[AI] {answer2}") # 이제 'it'을 이해하고 답변 가능

# 세션 기록 확인 (선택 사항)
print("\n--- 세션 기록 (user123) ---")
print(get_session_history(session_id_1).messages)

# 다른 세션 ID로 대화 (독립적인 기록 유지)
session_id_2 = "user456"
print("\n--- 새 대화 시작 (세션 ID: user456) ---")
question3 = "Tell me about Vector Stores in LangChain."
print(f"\n[User] {question3}")
answer3 = conversational_rag_chain.invoke(
    {"question": question3},
    config={"configurable": {"session_id": session_id_2}}
)
print(f"[AI] {answer3}") # user123의 대화 내용과 무관
```

*Self-Correction:* The initial thought was simpler integration, but proper RAG with memory often involves a "contextualization" step: taking the chat history and the new question to formulate a standalone question for the retriever. The code above implements this pattern. It first checks if there's history. If so, it uses `contextualize_q_chain` to potentially rephrase the question. Then, the potentially rephrased question is used in the main `rag_chain_core`. The history is passed to *both* the contextualization prompt and the final QA prompt. `RunnableWithMessageHistory` handles the loading and saving of history based on the `session_id` provided in the `config`.

이 코드를 실행하면, `session_id_1`을 사용하는 두 번째 질문("Who created it?")에서 챗봇이 첫 번째 질문("What is LangChain?")의 맥락을 이해하고 답변하는 것을 볼 수 있습니다. 또한 `session_id_2`는 `session_id_1`과 완전히 독립적인 대화 기록을 유지합니다.

### 정리 및 다음 단계

이번 8편에서는 챗봇의 '기억' 문제를 해결하는 LangChain의 **Memory** 컴포넌트를 탐구했습니다.

* **Memory의 필요성:** 자연스러운 대화 맥락 유지를 위한 Memory의 중요성을 이해했습니다.
* **다양한 Memory 종류:** `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationSummaryMemory` 등의 특징과 장단점을 살펴보았습니다.
* **LCEL과 Memory 통합:** 상태 비저장 LCEL Chain에 기억력을 부여하는 핵심 도구인 **`RunnableWithMessageHistory`** 의 사용법을 배우고, 세션별 대화 기록 관리 방법을 확인했습니다.
* **RAG 챗봇 업그레이드:** 기존 RAG Chain에 `RunnableWithMessageHistory`를 적용하여 대화 맥락을 기억하는 Conversational RAG Chain을 구축했습니다.

이제 우리는 단순히 질문에 답하는 것을 넘어, 사용자와의 대화 흐름을 기억하고 활용하는 한 단계 더 발전된 AI 애플리케이션을 만들 수 있게 되었습니다.

다음 **[9편]** 에서는 더욱 흥미로운 주제인 **Agent** 에 대해 알아봅니다! Agent는 LLM이 단순히 주어진 정보로 답변하는 것을 넘어, 스스로 **생각하고(Reasoning)** , **도구(Tools)** 를 사용하여 필요한 정보를 찾거나 작업을 수행하며 목표를 달성하는 자율적인 AI 에이전트를 구축하는 방법을 다룹니다. 마치 스스로 인터넷 검색을 하거나 계산기를 사용하는 AI 비서처럼 말이죠! 기대하셔도 좋습니다.

다양한 Memory 종류를 직접 적용해보고, 대화 기록이 어떻게 저장되고 활용되는지 관찰해보세요. 질문이 있다면 언제든지 댓글로 남겨주시기 바랍니다.