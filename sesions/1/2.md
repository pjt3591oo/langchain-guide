네, LangChain 블로그 시리즈의 두 번째 포스팅을 작성해 드리겠습니다.

---

## [LangChain 블로그 2편] LangChain의 핵심 구성 요소: Models, Prompts, Output Parsers

안녕하세요! LangChain 블로그 시리즈 2편입니다. 지난 [1편]에서는 LLM 애플리케이션 개발의 어려움과 이를 해결하기 위한 LangChain의 필요성 및 기본 개념에 대해 알아보았습니다.

이번 편에서는 LangChain을 구성하는 가장 기본적인 벽돌, 즉 **핵심 구성 요소(Core Components)** 인 **Models**, **Prompts**, **Output Parsers**에 대해 자세히 알아보고, 실제 Python 코드를 통해 어떻게 사용하는지 살펴보겠습니다. 이 세 가지 요소만 잘 이해해도 간단한 LLM 애플리케이션을 만들 수 있습니다!

**1. Models: LLM과의 연결고리**

LangChain의 `Model`은 다양한 종류의 언어 모델(LLM)과 상호작용할 수 있는 표준 인터페이스를 제공합니다. 덕분에 우리는 OpenAI의 GPT 모델, Hugging Face의 오픈소스 모델 등 다양한 모델을 일관된 방식으로 사용할 수 있습니다.

LangChain에는 크게 두 가지 유형의 모델 인터페이스가 있습니다.

* **LLMs (Large Language Models):** 텍스트 문자열을 입력으로 받아 텍스트 문자열을 출력하는 간단한 인터페이스입니다.
* **ChatModels (Chat Models):** 여러 개의 "채팅 메시지(System, Human, AI)" 리스트를 입력으로 받아 하나의 "채팅 메시지(주로 AI)"를 출력하는 인터페이스입니다. 최신 LLM들은 대부분 채팅 형식에 최적화되어 있어 `ChatModel` 인터페이스가 더 자주 사용됩니다.

**예제: OpenAI 챗 모델 사용하기 (`ChatOpenAI`)**

가장 널리 사용되는 OpenAI의 GPT 모델을 `ChatModel` 인터페이스로 사용해 보겠습니다.

```python
# 필요한 라이브러리 설치
# pip install langchain langchain-openai python-dotenv

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# .env 파일에서 환경 변수 로드 (API 키 관리)
# .env 파일에 OPENAI_API_KEY="your_api_key" 형식으로 저장
load_dotenv()

# ChatOpenAI 모델 초기화
# temperature는 생성 결과의 무작위성 조절 (0: 결정적, 높을수록 무작위)
# 모델 이름 지정 가능 (예: "gpt-4", "gpt-3.5-turbo")
chat_model = ChatOpenAI(temperature=0.7, model_name="gpt-3.5-turbo")

# 모델에 메시지 전달 및 응답 받기
# SystemMessage: 챗봇의 역할이나 지침 설정
# HumanMessage: 사용자의 입력
messages = [
    SystemMessage(content="You are a helpful assistant that translates English to Korean."),
    HumanMessage(content="Translate this sentence from English to Korean: I love programming."),
]

# invoke 메서드로 단일 응답 얻기
ai_response = chat_model.invoke(messages)

print(ai_response)
# 출력 예시: AIMessage(content='저는 프로그래밍을 사랑합니다.', response_metadata=...)
print(ai_response.content)
# 출력 예시: 저는 프로그래밍을 사랑합니다.
```

**주의:** 위 코드에서 `load_dotenv()`를 사용하여 `.env` 파일에서 OpenAI API 키를 안전하게 로드했습니다. 코드에 직접 API 키를 넣는 것은 보안상 좋지 않으므로, 환경 변수를 사용하는 것을 강력히 권장합니다.

**(참고) 다른 모델 사용:**
Hugging Face Hub의 모델을 사용하고 싶다면 `langchain-huggingface` 라이브러리를 설치하고 `HuggingFaceEndpoint`나 `HuggingFacePipeline` 등을 사용할 수 있습니다.

```python
# 예시 (Hugging Face Hub 사용 시)
# pip install langchain-huggingface huggingface_hub
# from langchain_huggingface import HuggingFaceEndpoint
# hf_model = HuggingFaceEndpoint(repo_id="google/flan-t5-xxl", huggingfacehub_api_token="YOUR_HF_TOKEN")
# response = hf_model.invoke("Translate English to French: How are you?")
# print(response)
```

**2. Prompts: LLM에게 원하는 것을 명확하게 지시하기**

LLM은 강력하지만, 우리가 원하는 결과물을 얻으려면 **명확하고 효과적인 지시(Prompt)** 가 필수적입니다. `Prompt`는 단순히 질문 문장을 넘어서, LLM이 특정 역할을 수행하도록 유도하거나, 주어진 형식에 맞춰 답변하도록 안내하는 역할을 합니다.

LangChain은 이러한 프롬프트를 동적으로 생성하고 관리할 수 있는 `PromptTemplate` 기능을 제공합니다.

* **`PromptTemplate`:** 간단한 텍스트 입력을 받아 LLM 인터페이스에 맞는 프롬프트를 생성합니다.
* **`ChatPromptTemplate`:** 채팅 모델 인터페이스에 맞게 하나 이상의 메시지(System, Human, AI)로 구성된 프롬프트를 생성합니다.

**예제 1: `PromptTemplate` 사용하기 (LLM 인터페이스용)**

```python
from langchain.prompts import PromptTemplate

# 템플릿 정의: {product} 부분에 변수가 들어감
prompt_template = PromptTemplate.from_template(
    "Suggest a good name for a company that makes {product}."
)

# 템플릿에 변수 값 채우기
formatted_prompt = prompt_template.format(product="colorful socks")

print(formatted_prompt)
# 출력 예시: Suggest a good name for a company that makes colorful socks.

# (만약 LLM 인터페이스 모델이 있다면)
# llm = SomeLLM() # LLM 인터페이스 모델 가정
# response = llm.invoke(formatted_prompt)
# print(response)
```

**예제 2: `ChatPromptTemplate` 사용하기 (ChatModel 인터페이스용)**

```python
from langchain.prompts import ChatPromptTemplate

# 채팅 메시지 템플릿 정의
chat_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful AI bot. Your name is {ai_name}."),
        ("human", "Hello, how are you doing?"),
        ("ai", "I'm doing well, thanks!"),
        ("human", "{user_input}"), # 사용자의 실제 입력이 들어갈 부분
    ]
)

# 템플릿에 변수 값 채워서 메시지 리스트 생성
messages = chat_template.format_messages(ai_name="Bob", user_input="What is your name?")

print(messages)
# 출력 예시:
# [SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),
#  HumanMessage(content='Hello, how are you doing?'),
#  AIMessage(content="I'm doing well, thanks!"),
#  HumanMessage(content='What is your name?')]

# 위 messages를 chat_model.invoke()에 전달하면 됨
ai_response = chat_model.invoke(messages)
print(ai_response.content)
# 출력 예시: My name is Bob.
```

**3. Output Parsers: LLM의 응답을 원하는 형태로 가공하기**

LLM의 응답은 주로 자연어 텍스트 형태입니다. 하지만 애플리케이션에서는 이 응답을 특정 구조(예: JSON, 리스트, 숫자)로 변환하여 사용해야 하는 경우가 많습니다. `Output Parser`는 LLM의 텍스트 응답을 개발자가 원하는 형식으로 파싱(parsing)하고 구조화하는 역할을 합니다.

Output Parser는 두 가지 주요 기능을 수행합니다.

1.  **형식 지정 지침(Format Instructions):** LLM이 원하는 출력 형식에 맞춰 응답하도록 프롬프트에 포함될 지침을 제공합니다.
2.  **파싱(Parsing):** LLM의 실제 응답(텍스트)을 받아 원하는 형식(예: JSON 객체, 리스트)으로 변환합니다.

**예제 1: 쉼표로 구분된 리스트 받기 (`CommaSeparatedListOutputParser`)**

```python
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import PromptTemplate

# 1. Output Parser 생성
output_parser = CommaSeparatedListOutputParser()

# 2. Parser의 형식 지정 지침 가져오기
format_instructions = output_parser.get_format_instructions()
print(format_instructions)
# 출력 예시: Your response should be a list of comma separated values, eg: `foo, bar, baz`

# 3. 프롬프트 템플릿에 지침 포함
prompt = PromptTemplate(
    template="List 5 ice cream flavors.\n{format_instructions}",
    input_variables=[],
    partial_variables={"format_instructions": format_instructions}, # 프롬프트에 지침 주입
)

# 4. 모델 호출 (프롬프트 포맷팅 포함)
formatted_prompt = prompt.format()
ai_response = chat_model.invoke(formatted_prompt)
output = ai_response.content
print("모델 응답:", output)
# 출력 예시: 모델 응답: Vanilla, Chocolate, Strawberry, Mint Chocolate Chip, Cookies and Cream

# 5. Parser로 결과 파싱
parsed_output = output_parser.parse(output)
print("파싱된 결과:", parsed_output)
# 출력 예시: 파싱된 결과: ['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookies and Cream']
# 이제 리스트로 다룰 수 있음!
print("첫 번째 맛:", parsed_output[0])
```

**예제 2: JSON 형식으로 받기 (`JsonOutputParser` & Pydantic)**

Pydantic 모델을 사용하여 더 안정적으로 JSON 출력을 파싱할 수 있습니다.

```python
# 필요한 라이브러리 설치
# pip install pydantic

from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List

# 1. 원하는 출력 형식을 Pydantic 모델로 정의
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

# 2. Output Parser 생성 (Pydantic 모델 지정)
pydantic_parser = PydanticOutputParser(pydantic_object=Joke)

# 3. Parser의 형식 지정 지침 가져오기
format_instructions = pydantic_parser.get_format_instructions()
print(format_instructions)
# 출력 예시: (JSON 스키마와 함께 출력 지침이 나옴)
# The output should be formatted as a JSON instance that conforms to the JSON schema below.
# As an example, for the schema {"title": "foo", "description": "bar"}
# the object {"title": "foo", "description": "bar"} is a well-formatted instance of the schema.
# The object {"title": "foo"} is not well-formatted.
#
# Here is the output schema:
# ```json
# {"properties": {"setup": {"title": "Setup", "description": "question to set up a joke", "type": "string"}, "punchline": {"title": "Punchline", "description": "answer to resolve the joke", "type": "string"}}, "required": ["setup", "punchline"]}
# ```

# 4. 프롬프트 템플릿에 지침 포함
prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": format_instructions}
)

# 5. 모델 호출 (프롬프트 포맷팅 포함)
query = "Tell me a joke."
formatted_prompt = prompt.format(query=query)
ai_response = chat_model.invoke(formatted_prompt)
output = ai_response.content
print("모델 응답 (JSON 문자열):", output)
# 출력 예시: 모델 응답 (JSON 문자열): ```json
# {
#	"setup": "Why don't scientists trust atoms?",
#	"punchline": "Because they make up everything!"
# }
# ```

# 6. Parser로 결과 파싱 (JSON 문자열 -> Pydantic 객체)
try:
    parsed_joke = pydantic_parser.parse(output)
    print("파싱된 결과 (Pydantic 객체):", parsed_joke)
    # 출력 예시: 파싱된 결과 (Pydantic 객체): setup="Why don't scientists trust atoms?" punchline="Because they make up everything!"
    print("농담 설정:", parsed_joke.setup)
except Exception as e:
    print("파싱 오류:", e)

```

**핵심 요소들의 연결 (살짝 맛보기)**

지금까지 살펴본 Models, Prompts, Output Parsers는 각각 독립적으로도 유용하지만, LangChain의 진정한 힘은 이들을 **연결(Chaining)** 할 때 발휘됩니다. LangChain Expression Language (LCEL)을 사용하면 이 연결을 매우 간결하게 표현할 수 있습니다.

```python
# LCEL 예시 (다음 편에서 자세히!)
# chain = prompt | chat_model | pydantic_parser
# result = chain.invoke({"query": "Tell me a joke."})
# print(result.punchline)
```

위 코드는 프롬프트 생성, 모델 호출, 출력 파싱의 과정을 `|` (파이프) 연산자로 자연스럽게 연결한 모습입니다. 다음 편에서 이 LCEL과 Chain에 대해 본격적으로 다룰 예정입니다!

**마무리하며**

이번 편에서는 LangChain의 심장과도 같은 세 가지 핵심 구성 요소인 **Models, Prompts, Output Parsers**에 대해 알아보았습니다.

* **Models:** 다양한 LLM을 일관되게 사용하는 인터페이스
* **Prompts:** LLM에게 효과적으로 지시하는 템플릿
* **Output Parsers:** LLM의 응답을 구조화된 데이터로 변환하는 도구

이들은 LangChain 애플리케이션을 만드는 데 있어 가장 기본적인 재료들입니다. 다음 편에서는 이 재료들을 조합하여 본격적인 워크플로우를 만드는 **Chain**과 **LCEL(LangChain Expression Language)** 에 대해 깊이 있게 알아보겠습니다.

코드 예제를 직접 실행해보시고 궁금한 점은 댓글로 남겨주세요!

---

두 번째 포스팅이 완료되었습니다. 이어서 **[3편] LangChain의 심장, Chain: 단순 호출을 넘어 워크플로우 구축하기 (LCEL 포함)** 포스팅을 작성해 드릴까요?