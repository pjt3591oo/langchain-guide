네, LangChain 블로그 시리즈의 네 번째 포스팅을 작성해 드리겠습니다.

---

## [LangChain 블로그 4편] LCEL 심화: Runnable 인터페이스와 다양한 Chain 조합

안녕하세요! LangChain 블로그 시리즈 4편입니다. 지난 [3편]에서는 LangChain의 핵심 조합 방식인 **LCEL(LangChain Expression Language)** 과 파이프(`|`) 연산자를 이용한 기본적인 Chain 구성법을 알아보았습니다. LCEL 덕분에 우리는 Model, Prompt, Parser와 같은 컴포넌트들을 놀랍도록 간결하게 연결할 수 있었죠.

이번 편에서는 LCEL을 한 단계 더 깊이 파고들어, **`Runnable` 인터페이스의 다양한 메서드**들을 활용하는 방법과 **`RunnablePassthrough`, `RunnableParallel`** 과 같은 특수 Runnable들을 사용하여 **더 복잡하고 유연한 데이터 흐름을 제어하는 기술**에 대해 자세히 알아보겠습니다.

**1. Runnable 인터페이스 깊이 보기: `invoke`를 넘어서**

LCEL의 모든 컴포넌트는 `Runnable` 프로토콜을 따릅니다. 지난 편에서는 주로 `invoke()` 메서드를 사용하여 Chain을 실행하고 최종 결과를 한 번에 받았지만, `Runnable`은 더 다양한 실행 방식을 제공합니다.

* **`invoke(input)`:** 단일 입력을 받아 Chain을 실행하고 최종 결과 반환 (동기 방식). 가장 기본적인 실행 메서드입니다.
* **`batch(inputs: List)`:** 여러 개의 입력 리스트를 받아 병렬로 처리하고 결과 리스트 반환 (동기 방식). 여러 작업을 한 번에 효율적으로 처리할 때 유용합니다.
* **`stream(input)`:** 단일 입력을 받아 최종 결과가 아닌, 중간 처리 과정의 청크(chunk)들을 실시간 스트림으로 반환 (동기 방식). LLM 응답을 실시간으로 보여주는 UI 등에 활용됩니다.
* **`ainvoke(input)`, `abatch(inputs: List)`, `astream(input)`:** 위 메서드들의 비동기(asynchronous) 버전입니다. FastAPI, Streamlit 등 비동기 프레임워크 환경에서 사용하면 블로킹 없이 효율적인 I/O 처리가 가능합니다.

**예제: `batch` 와 `stream` 활용**

(3편에서 정의한 `chain_lcel = prompt | chat_model | pydantic_parser` 와 `Joke` Pydantic 모델을 재사용합니다.)

```python
# 필요한 라이브러리 및 설정 (이전 포스팅 내용)
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain.schema import StrOutputParser
from pydantic import BaseModel, Field

load_dotenv()
chat_model = ChatOpenAI(temperature=0.7, model_name="gpt-3.5-turbo")

class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")
pydantic_parser = PydanticOutputParser(pydantic_object=Joke)
format_instructions = pydantic_parser.get_format_instructions()

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI that tells jokes based on a topic."),
    ("human", "Tell me a joke about {topic}.\n{format_instructions}")
])

chain_lcel = prompt | chat_model | pydantic_parser

# --- batch 예제 ---
print("\n--- Batch Execution Start ---")
topics_to_process = [
    {"topic": "computers", "format_instructions": format_instructions},
    {"topic": "coffee", "format_instructions": format_instructions},
]

# batch 메서드로 여러 입력 동시 처리 (내부적으로 병렬 처리될 수 있음)
batch_results = chain_lcel.batch(topics_to_process)

print(batch_results)
# 출력 예시: [Joke(setup='Why did the computer keep sneezing?', punchline='It had a virus!'), Joke(setup='How does Moses make coffee?', punchline='He brews it.')]
print("Batch 결과 개수:", len(batch_results))
print("첫 번째 농담:", batch_results[0].punchline)
print("--- Batch Execution End ---")


# --- stream 예제 (StrOutputParser 사용) ---
stream_chain = prompt | chat_model | StrOutputParser() # 스트리밍을 위해 문자열 파서 사용

print("\n--- Streaming Execution Start ---")
topic_to_stream = {"topic": "books", "format_instructions": format_instructions}

# stream 메서드로 결과 실시간 받기
full_response = ""
for chunk in stream_chain.stream(topic_to_stream):
    print(chunk, end="", flush=True)
    full_response += chunk # 필요하다면 청크를 모아서 전체 응답 구성 가능

print("\n--- Streaming Execution End ---")
# print("Full streamed response:", full_response) # 전체 응답 확인
```

**(참고) 비동기 메서드:**
비동기 환경(예: `async def` 함수 내)에서는 `await chain_lcel.ainvoke(...)`, `await chain_lcel.abatch(...)`, `async for chunk in chain_lcel.astream(...)` 와 같이 `a` 접두사가 붙은 메서드를 사용합니다.

**2. 데이터 흐름 제어하기: `RunnablePassthrough` 와 `RunnableParallel`**

LCEL의 진정한 강력함은 복잡한 데이터 흐름을 제어할 때 드러납니다. 단순히 일렬로 연결하는 것을 넘어, 특정 데이터를 유지하거나 여러 작업을 병렬로 실행해야 할 때가 있습니다. 이때 `RunnablePassthrough`와 `RunnableParallel`이 유용합니다.

* **`RunnablePassthrough`:** 입력을 **변경 없이 그대로 다음 단계로 전달**합니다. 주로 `RunnableParallel`과 함께 사용되어, 병렬 처리 중에도 원본 입력의 일부를 유지하고 싶을 때 사용됩니다.

* **`RunnableParallel`:** 여러 개의 Runnable (또는 Runnable Map)을 **동시에 실행**하고, 그 결과를 **딕셔너리 형태**로 묶어서 반환합니다. 입력 딕셔너리의 각 키에 대해 서로 다른 Runnable을 매핑하거나, 같은 입력을 여러 Runnable에 동시에 전달할 수 있습니다.

**예제 1: `RunnablePassthrough` - 원본 질문 유지하기 (RAG 준비)**

나중에 만들 RAG(Retrieval-Augmented Generation) 시스템을 생각해보면, 사용자 질문(`question`)으로 관련 문서(`context`)를 검색한 뒤, **원본 질문과 검색된 문서를 함께** LLM 프롬프트에 넣어야 합니다. 이때 `RunnablePassthrough`가 유용합니다.

```python
from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda
# from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

# 가상의 Retriever 함수 (실제로는 Vector Store 등에서 검색)
def retrieve_context(query: str) -> str:
    print(f"[Debug] Retrieving context for: {query}")
    # 실제 구현에서는 관련 문서를 찾아서 반환
    if "LCEL" in query:
        return "LCEL stands for LangChain Expression Language. It allows composing chains using the | operator."
    return "No specific context found."

# Retriever를 Runnable로 변환 (간단히 함수를 감싸서)
retriever = RunnableLambda(retrieve_context)

# RunnableParallel과 Passthrough 사용
# 입력: {"question": "What is LCEL?"}
# 출력: {"context": "LCEL stands for...", "question": "What is LCEL?"}
setup_and_retrieval = RunnableParallel(
    # retriever는 question을 입력받아 context를 생성
    context=retriever,
    # RunnablePassthrough는 입력을 그대로 전달 (여기서는 입력 딕셔너리 전체)
    # 여기서는 question만 필요하므로, itemgetter를 사용하거나 lambda를 쓸 수도 있음
    # 가장 간단한 형태는 입력 자체를 통과시키는 것. 후속 단계에서 필요한 키를 사용.
    question=RunnablePassthrough()
)

# RAG 프롬프트 템플릿
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the question based only on the following context:\n{context}"),
    # RunnableParallel의 출력에서 question을 사용하기 위해 input_data.question 형태로 접근
    # 또는 setup_and_retrieval 출력 후 후처리 단계에서 prompt 입력을 재구성할 수도 있음.
    # 여기서는 간단하게 프롬프트 내에서 직접 접근하는 방식을 보여줍니다.
    # **수정**: RunnableParallel 출력의 'input_data' 키 아래에 원본 입력이 들어갑니다.
    ("human", "{question}")
])

# RAG Chain (아직 모델과 파서는 연결 안 함)
# setup_and_retrieval의 출력: {"context": ..., "input_data": {"question": ...}}
# 이 출력이 rag_prompt로 전달되어 포맷팅됨
rag_chain_part1 = setup_and_retrieval | rag_prompt

# 실행 테스트
result_messages = rag_chain_part1.invoke({"question": "What is LCEL?"})
print("\n--- RAG Prompt Messages ---")
print(result_messages.to_messages())
# 출력 예시:
# --- RAG Prompt Messages ---
# [SystemMessage(content='Answer the question based only on the following context:\nLCEL stands for LangChain Expression Language. It allows composing chains using the | operator.'), HumanMessage(content='What is LCEL?')]
```
*(코드 수정: `RunnableParallel` 출력에서 `RunnablePassthrough` 결과를 어떻게 참조하는지 명확히 하고, `RunnableLambda` 사용 예시 추가)*

**예제 2: `RunnableParallel` - 여러 작업 병렬 실행**

하나의 주제에 대해 농담 생성과 간단한 설명을 동시에 생성하고 싶다고 가정해 봅시다.

```python
from langchain_core.runnables import RunnableParallel
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain.schema import StrOutputParser
from pydantic import BaseModel, Field

chat_model = ChatOpenAI(temperature=0.7, model_name="gpt-3.5-turbo")

class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")
pydantic_parser = PydanticOutputParser(pydantic_object=Joke)
format_instructions = pydantic_parser.get_format_instructions()

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI that tells jokes based on a topic."),
    ("human", "Tell me a joke about {topic}.\n{format_instructions}")
])

chain_lcel = prompt | chat_model | pydantic_parser


# 농담 생성 Chain (위에서 정의한 chain_lcel 재사용)
joke_chain = prompt | chat_model | pydantic_parser

# 간단 설명 생성 Chain
explanation_prompt = ChatPromptTemplate.from_template(
    "Briefly explain what {topic} is in one sentence."
)
explanation_chain = explanation_prompt | chat_model | StrOutputParser()

# 두 Chain을 병렬로 실행하는 Combined Chain
# 입력: {"topic": "..."}
# 출력: {"joke": Joke(...), "explanation": "..."}
combined_chain = RunnableParallel(
    joke=joke_chain,
    explanation=explanation_chain,
)

# 실행
topic = "AI"
# RunnableParallel은 각 Runnable에 필요한 입력 키를 자동으로 전달합니다.
# joke_chain과 explanation_chain 모두 'topic' 변수가 필요합니다.
# format_instructions는 joke_chain에만 필요하므로, partial을 사용하거나
# RunnableParallel의 각 항목에 lambda를 써서 입력을 재구성할 수 있습니다.
# 여기서는 joke_chain이 format_instructions를 필요로 하므로, 입력을 조금 수정합니다.

combined_result = combined_chain.invoke({
    "topic": topic,
    "format_instructions": format_instructions # joke_chain을 위해 전달
})

print("\n--- Parallel Execution Result ---")
print("Generated Joke:", combined_result['joke'])
print("Generated Explanation:", combined_result['explanation'])
# 출력 예시:
# --- Parallel Execution Result ---
# Generated Joke: setup='Why did the AI break up with the computer?' punchline="Because it said it needed space!"
# Generated Explanation: AI, or Artificial Intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems.
```
*(코드 수정: `RunnableParallel` 입력이 각 하위 Runnable의 요구사항을 어떻게 충족하는지 명확히 함. `format_instructions` 추가)*

**3. 복잡한 워크플로우 구축하기**

이제 `RunnablePassthrough`, `RunnableParallel` 그리고 기본 파이프(`|`) 연산자를 조합하면 매우 복잡하고 정교한 워크플로우도 LCEL로 깔끔하게 표현할 수 있습니다. 예를 들어 위에서 맛본 RAG Chain을 완성해볼 수 있습니다.

```python
from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

chat_model = ChatOpenAI(
    base_url="http://localhost:1234/v1",
    temperature=0.7, 
    model_name="l3-8b-stheno-v3.1-iq-imatrix"
)
# 가상의 Retriever 함수 (실제로는 Vector Store 등에서 검색)
def retrieve_context(query: str) -> str:
    print(f"[Debug] Retrieving context for: {query}")
    # 실제 구현에서는 관련 문서를 찾아서 반환
    if "LCEL" in query:
        return "LCEL stands for LangChain Expression Language. It allows composing chains using the | operator."
    return "No specific context found."

# Retriever를 Runnable로 변환 (간단히 함수를 감싸서)
retriever = RunnableLambda(retrieve_context)

rag_chain_final = (
    # 1단계: Context 검색 및 원본 질문 유지 (병렬 처리)
    # promps에 포함될 내용 생성
    RunnableParallel(
        context=retriever,
        question=RunnablePassthrough() # 입력의 question만 통과시키도록 수정
        # question = lambda x: x["question"] # 이렇게 명시적으로 지정 가능
    )
    # 2단계: 검색된 context와 원본 question으로 프롬프트 생성
    | ChatPromptTemplate.from_messages([
          ("system", "Answer the question based only on the following context:\n{context}"),
          ("human", "{question}") # Passthrough된 딕셔너리에서 question 키 값 사용
      ])
    # 3단계: 모델 호출
    | chat_model
    # 4단계: 결과 파싱 (문자열로)
    | StrOutputParser()
)

print("\n--- Full RAG Chain Execution ---")
final_answer = rag_chain_final.invoke({"question": "What does LCEL stand for?"})
print(final_answer)
# 출력 예시:
# [Debug] Retrieving context for: What does LCEL stand for?
# LCEL stands for LangChain Expression Language.
```
*(코드 수정: `RunnableParallel`에서 `question` 처리 방식 명확화 및 `RunnableParallel` 출력 형태에 맞춰 후속 프롬프트에서 `question[question]`으로 접근하도록 수정)*


**마무리하며**

이번 편에서는 LCEL의 강력한 기능들을 더 깊이 탐구했습니다. `Runnable` 인터페이스의 `batch`, `stream`, 비동기 메서드들을 통해 실행 효율성을 높일 수 있으며, `RunnablePassthrough`와 `RunnableParallel`을 사용하여 복잡한 데이터 흐름을 명확하고 유연하게 제어할 수 있음을 확인했습니다.

LCEL은 단순히 코드를 짧게 만드는 것을 넘어, **생각하는 방식대로 Chain을 구성**할 수 있게 해주는 강력한 도구입니다. 이제 여러분은 LangChain으로 훨씬 더 정교하고 복잡한 LLM 애플리케이션을 만들 수 있는 기반을 다졌습니다!

**다음 편 예고:**

다음 [5편]부터는 LangChain의 가장 대표적인 활용 사례 중 하나인 **RAG(Retrieval-Augmented Generation)** 구축 여정을 시작합니다. 첫 단계로, **외부 데이터를 LangChain으로 가져오고(Document Loading), LLM이 처리하기 좋은 형태로 분할하는(Text Splitting)** 방법에 대해 자세히 알아보겠습니다.

복잡한 Chain 구성이 흥미로우셨다면 댓글로 의견을 나눠주세요!

---

네 번째 포스팅이 완료되었습니다. 이어서 **[5편] 나만의 데이터와 대화하기 (1): 문서 로드 및 분할 (Document Loading & Splitting)** 포스팅을 작성해 드릴까요?