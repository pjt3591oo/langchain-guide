## LangChain 블로그 시리즈 [12편]: 내 LangChain 앱을 API로! LangServe를 이용한 간편 배포

안녕하세요! LangChain 여정의 12번째 시간에 오신 것을 환영합니다. 우리는 [11편]에서 LangSmith를 통해 복잡한 LangChain 애플리케이션의 내부를 들여다보고 디버깅하는 방법을 배웠습니다. 이제 우리가 열심히 만든 Chain, RAG 시스템, Agent 등을 우리 자신만 사용하는 것을 넘어, 다른 사용자나 다른 서비스가 쉽게 접근하고 활용할 수 있도록 만들 차례입니다. 이를 위한 가장 표준적인 방법은 바로 **API(Application Programming Interface)**로 만드는 것입니다.

"내가 만든 LangChain 앱을 어떻게 API 서버로 만들지?" 고민하셨다면, **LangServe**가 그 해답을 제시합니다. LangServe는 LangChain에서 공식적으로 제공하는 라이브러리로, 여러분이 만든 모든 LangChain `Runnable` 객체를 **단 몇 줄의 코드만으로 강력하고 표준적인 REST API로 배포**할 수 있게 해줍니다. 마치 마법처럼 느껴질 정도로 간편합니다!

이번 시간에는 LangServe를 사용하여 우리가 만든 LangChain 애플리케이션을 실제 API 서비스로 만들어보는 과정을 함께 하겠습니다.

---

**🎯 이번 시간에 배울 내용:**

1.  **LangServe란 무엇인가? 왜 사용해야 할까?** (LangServe의 장점)
2.  **LangServe 설치 및 기본 설정** (`pip install`, 서버 코드 작성)
3.  **LangServe 서버 실행 및 API 테스트** (`uvicorn` 실행, `requests` 호출, Playground UI)
4.  **(선택) 기존 FastAPI 앱에 LangServe 추가하기**

---

### 1. LangServe란 무엇인가? 왜 사용해야 할까?

**LangServe**는 LangChain `Runnable` 인터페이스를 따르는 모든 객체(Chain, AgentExecutor, Retriever 등)를 **REST API 엔드포인트로 노출**시켜주는 파이썬 라이브러리입니다. 즉, 복잡한 웹 프레임워크 코드를 직접 작성할 필요 없이, 여러분의 LangChain 로직을 바로 API 서비스로 전환할 수 있습니다.

**왜 LangServe를 사용해야 할까요?**

* **🚀 간편함 (Simplicity):** 단 몇 줄의 코드로 어떤 `Runnable`이든 API로 만들 수 있습니다. 웹 개발 경험이 적어도 쉽게 시작할 수 있습니다.
* **มาตรฐาน (Standardization):** LangChain 앱을 배포하는 표준적이고 일관된 방법을 제공합니다.
* **🤖 자동 스키마 생성 (Automatic Schema Generation):** `Runnable`의 입력 및 출력 타입 힌트(Type Hint)를 기반으로 자동으로 API 입출력 스키마(Pydantic 모델)를 생성합니다. 이를 통해 요청/응답 데이터 유효성 검사 및 API 문서 생성이 용이해집니다.
* **⚡ FastAPI 기반 (Built on FastAPI):** 빠르고 현대적인 파이썬 웹 프레임워크인 FastAPI를 기반으로 구축되어 성능과 안정성이 뛰어납니다. 비동기(async) 처리를 완벽하게 지원합니다.
* **🌊 스트리밍 및 배치 지원 (Streaming & Batch Support):** 기본 `Runnable`이 스트리밍(`stream`)이나 배치(`batch`)를 지원한다면, LangServe는 `/stream`, `/batch` 엔드포인트를 자동으로 생성하여 해당 기능을 API 레벨에서 제공합니다.
* **🎮 내장 플레이그라운드 (Built-in Playground):** 배포된 API를 웹 브라우저에서 바로 테스트해볼 수 있는 대화형 UI를 자동으로 제공합니다.
* **🛠️ 프로덕션 준비 완료 (Production Ready):** 로깅, 예외 처리 등 실제 서비스 운영에 필요한 기능들을 포함하고 있습니다.

복잡한 설정 없이 LangChain 애플리케이션을 빠르고 안정적으로 서비스화하고 싶다면 LangServe는 최고의 선택입니다.

### 2. LangServe 설치 및 기본 설정

LangServe를 사용하기 위한 설치와 기본 서버 코드 작성은 매우 간단합니다.

**1. 설치:**

터미널에서 다음 명령어를 실행하여 LangServe 및 관련 의존성(FastAPI, Uvicorn 등)을 설치합니다.

```bash
pip install "langchain[server]"
```

`[server]` 부분을 포함해야 필요한 웹 서버 관련 패키지들이 함께 설치됩니다.

**2. 기본 서버 코드 작성 (`server.py`):**

이제 배포하고 싶은 LangChain `Runnable`을 가져와 LangServe를 이용해 API 엔드포인트로 만드는 파이썬 스크립트(`server.py` 같은 이름으로 저장)를 작성해 봅시다. 여기서는 [7편]에서 만들었던 간단한 RAG Chain을 예시로 사용하겠습니다. (실제로는 여러분이 만든 어떤 `Runnable`이든 가능합니다.)

```python
# server.py

from fastapi import FastAPI
from langserve import add_routes
import uvicorn
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import TextLoader # 예시용 로더

# --- 0. (선택) 환경 변수 설정 ---
# import os
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"

# --- 1. 배포할 Runnable 생성 (예: 간단 RAG Chain) ---

# 가상의 문서 로드 및 분할 (실제 환경에서는 DB 등에서 가져올 수 있음)
try:
    # 예시 텍스트 파일 생성 (없을 경우)
    with open("example.txt", "w", encoding='utf-8') as f:
        f.write("LangServe는 LangChain Runnable을 API로 쉽게 배포하는 라이브러리입니다.\n")
        f.write("FastAPI를 기반으로 하며, 스트리밍과 배치 처리를 지원합니다.\n")
        f.write("LangSmith와 함께 사용하면 개발부터 배포, 모니터링까지 편리하게 관리할 수 있습니다.")

    loader = TextLoader("example.txt", encoding='utf-8')
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = text_splitter.split_documents(documents)

    # 임베딩 및 벡터 스토어 생성
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()

except Exception as e:
    print(f"초기 설정 중 오류 발생 (OpenAI 키 확인 또는 파일 경로 확인): {e}")
    # 실제 서비스에서는 더 강력한 오류 처리 필요
    # 여기서는 오류 시 기본적인 LLMChain으로 대체
    retriever = None # Retriever 설정 실패 표시


# 모델 및 프롬프트 설정
model = ChatOpenAI()
prompt = ChatPromptTemplate.from_template("""주어진 내용을 바탕으로 질문에 답해주세요.
내용: {context}
질문: {question}
답변:""")

# RAG Chain 정의
if retriever:
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | model
        | StrOutputParser()
    )
else:
    # Retriever 실패 시 간단한 LLMChain으로 대체 (예시)
    prompt_fallback = ChatPromptTemplate.from_template("{question} 에 대해 답해주세요.")
    rag_chain = (
        RunnablePassthrough() # 입력을 그대로 question으로 사용
        | prompt_fallback
        | model
        | StrOutputParser()
    )


# --- 2. FastAPI 앱 생성 ---
app = FastAPI(
    title="LangChain RAG Server",
    version="1.0",
    description="LangServe를 이용한 간단한 RAG API 서버",
)

# --- 3. LangServe를 이용해 Runnable을 API 라우트로 추가 ---
# add_routes 함수가 핵심입니다!
add_routes(
    app,
    rag_chain, # 배포할 Runnable 객체
    path="/rag-chain", # API 엔드포인트 경로 설정
)

# --- 4. (선택) 서버 실행 코드 (개발용) ---
if __name__ == "__main__":
    # 이 파일을 직접 실행할 때 uvicorn 서버를 시작합니다.
    # 실제 배포 환경에서는 gunicorn 등과 함께 사용될 수 있습니다.
    uvicorn.run(app, host="localhost", port=8000)

```

**코드 설명:**

1.  배포하고자 하는 `Runnable` 객체 (`rag_chain`)를 정의하거나 임포트합니다. 위 예시에서는 간단한 RAG Chain을 직접 정의했습니다. (Retriever 설정 실패 시 대체 Chain 포함)
2.  `FastAPI` 애플리케이션 인스턴스를 생성합니다. (`title`, `version`, `description`은 API 문서에 표시됩니다.)
3.  **`langserve.add_routes(app, rag_chain, path="/rag-chain")`** 가 핵심입니다. 이 함수는:
    * `app`: FastAPI 앱 인스턴스
    * `rag_chain`: 배포할 `Runnable` 객체
    * `path="/rag-chain"`: 이 `Runnable`을 서비스할 API 경로 (`http://.../rag-chain`)
    이 함수 호출 하나로 `/invoke`, `/batch`, `/stream`, `/stream_log`, `/playground` 등 다양한 엔드포인트가 `/rag-chain` 하위에 자동으로 생성됩니다.
4.  `(선택)` `if __name__ == "__main__":` 블록은 `python server.py` 명령으로 파일을 직접 실행했을 때 개발용 웹 서버(`uvicorn`)를 구동시키는 코드입니다.

### 3. LangServe 서버 실행 및 API 테스트

이제 작성한 `server.py`를 실행하고 API를 테스트해 보겠습니다.

**1. 서버 실행:**

터미널에서 `server.py` 파일이 있는 디렉토리로 이동한 후, 다음 명령어를 실행합니다.

```bash
uvicorn server:app --reload --host 0.0.0.0 --port 8000
```

* `server:app`: `server.py` 파일의 `app` (FastAPI 인스턴스)을 실행하라는 의미입니다.
* `--reload`: 개발 중에 코드가 변경되면 서버를 자동으로 재시작합니다. 프로덕션 환경에서는 제거합니다.
* `--host 0.0.0.0`: 서버가 로컬 머신의 모든 네트워크 인터페이스에서 요청을 수신하도록 합니다. (localhost 또는 127.0.0.1로 접속 가능)
* `--port 8000`: 서버가 8000번 포트를 사용하도록 합니다.

서버가 성공적으로 실행되면 터미널에 관련 로그가 출력됩니다.

**2. API 호출 (Python `requests` 사용):**

다른 파이썬 스크립트나 Jupyter 노트북에서 `requests` 라이브러리를 사용하여 배포된 API를 호출할 수 있습니다.

```python
import requests

# invoke: 단일 입력에 대한 결과 반환
response_invoke = requests.post(
    "http://localhost:8000/rag-chain/invoke",
    json={'input': 'LangServe는 무엇인가요?'} # Runnable의 입력 구조에 맞춰 json 전달
)
print("--- Invoke 결과 ---")
# LangServe는 출력 타입을 자동으로 파악하여 json으로 반환합니다.
# StrOutputParser를 사용했으므로 'output' 키에 문자열 결과가 들어있습니다.
print(response_invoke.json())


# batch: 여러 입력에 대한 결과 리스트 반환
response_batch = requests.post(
    "http://localhost:8000/rag-chain/batch",
    json={'inputs': ['LangServe는 무엇인가요?', 'LangSmith는 무엇인가요?']}
)
print("\n--- Batch 결과 ---")
print(response_batch.json())


# stream: 결과를 스트리밍 형태로 받기 (청크 단위)
response_stream = requests.post(
    "http://localhost:8000/rag-chain/stream",
    json={'input': 'LangServe의 장점은 무엇인가요?'},
    stream=True # 스트리밍 요청 설정
)
print("\n--- Stream 결과 ---")
for chunk in response_stream.iter_content(chunk_size=None):
    if chunk:
        # 스트리밍 응답은 일반적으로 byte 형태이므로 디코딩 필요
        # 출력 형식에 따라 파싱 방법이 다를 수 있음 (기본은 문자열 청크)
        try:
            # LangServe의 스트리밍은 Server-Sent Events (SSE) 형식을 따를 수 있음
            # 간단한 텍스트 스트림으로 가정하고 출력
            print(chunk.decode('utf-8'), end="")
        except UnicodeDecodeError:
            print("[binary chunk]", end="") # 디코딩 불가 청크
print("\n-------------------")
```

`invoke`, `batch`, `stream` 엔드포인트에 `Runnable`의 입력 스키마에 맞는 JSON 데이터를 POST 요청으로 보내면, LangServe가 이를 처리하고 결과를 JSON 형태로 반환합니다.

**3. Playground UI 사용:**

LangServe의 가장 멋진 기능 중 하나는 바로 내장된 웹 기반 테스트 인터페이스입니다. 웹 브라우저를 열고 다음 주소로 접속해 보세요.

`http://localhost:8000/rag-chain/playground/`

다음과 같은 화면을 볼 수 있습니다.

* **Input:** 여기에 질문을 입력합니다.
* **(Configuration):** 만약 `Runnable`이 설정 가능한 옵션(예: `configurable_fields`)을 가지고 있다면 여기에 표시됩니다.
* **Invoke / Stream 버튼:** 입력한 내용으로 API를 호출합니다.
* **Output:** API 응답 결과가 표시됩니다. 스트리밍의 경우 결과가 실시간으로 나타납니다.
* **Trace 링크:** 만약 [11편]에서 설정한 것처럼 LangSmith 추적이 활성화되어 있다면, 각 실행에 대한 LangSmith Trace 링크가 제공되어 바로 디버깅 화면으로 이동할 수 있습니다!

Playground는 개발 중 API를 빠르게 테스트하거나 다른 사람에게 기능을 시연할 때 매우 유용합니다.

**(참고: API 문서)**
FastAPI 기반이므로, `http://localhost:8000/docs` 로 접속하면 Swagger UI 형태의 자동 생성된 API 문서를 볼 수 있습니다. 각 엔드포인트의 상세 정보와 입출력 스키마를 확인할 수 있습니다.

### 4. (선택) 기존 FastAPI 앱에 LangServe 추가하기

만약 이미 운영 중인 FastAPI 애플리케이션이 있고, 여기에 LangChain 기반의 기능을 엔드포인트로 추가하고 싶다면 어떻게 할까요? 간단합니다. 기존 FastAPI `app` 인스턴스에 `add_routes`를 호출하기만 하면 됩니다.

```python
# existing_fastapi_app.py
from fastapi import FastAPI
# ... (기존 FastAPI 라우트 및 로직) ...

# LangChain Runnable 및 LangServe 관련 임포트
from langserve import add_routes
from langchain_core.runnables import RunnableLambda

# 기존 FastAPI 앱 인스턴스
app = FastAPI()

# --- 기존 API 라우트들 ---
@app.get("/")
def read_root():
    return {"message": "기존 FastAPI 앱입니다."}

# ... (다른 기존 라우트들) ...


# --- LangChain Runnable 정의 (예: 간단한 람다 함수) ---
def _add_one(x: int) -> int:
    return x + 1

add_one_runnable = RunnableLambda(_add_one)

# --- 기존 앱에 LangServe 라우트 추가 ---
add_routes(
    app,
    add_one_runnable,
    path="/add_one", # 기존 라우트와 겹치지 않는 경로 설정
)

# 서버 실행 (기존 방식대로)
# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="localhost", port=8000)
```

이렇게 하면 기존 API 엔드포인트 (`/`)와 LangServe가 추가한 엔드포인트 (`/add_one/invoke` 등)가 모두 같은 FastAPI 서버에서 서비스됩니다.

---

### 5. 마무리

이번 시간에는 LangChain 애플리케이션을 손쉽게 REST API로 배포할 수 있게 해주는 강력한 도구, **LangServe**에 대해 배웠습니다. 설치부터 서버 실행, API 테스트, 그리고 편리한 Playground UI까지 살펴보았습니다. LangServe를 사용하면 복잡한 웹 서버 코드를 작성하는 데 드는 시간을 절약하고, 핵심적인 LLM 애플리케이션 로직 개발에 더 집중할 수 있습니다.

이제 여러분이 만든 멋진 LangChain 애플리케이션을 세상에 선보일 준비가 되었습니다! [7편]의 RAG 챗봇이나 [10편]의 커스텀 도구를 가진 Agent를 LangServe로 직접 배포해 보세요.

다음 시간에는 **섹션 6: 심화 주제 및 실전 응용**으로 넘어갑니다. 그 첫 번째 주제로 **[13편] LangChain 실전 응용 사례 분석**에서는 문서 요약, 질문 답변 시스템 고도화 등 LangChain을 활용한 구체적인 응용 사례들을 더 깊이 있게 살펴보겠습니다.

LangServe 사용 중 궁금한 점이나 문제가 있다면 언제든지 댓글로 알려주세요!

---

**다음 편 예고:** [13편] LangChain 실전 응용 사례 분석 (문서 요약, Q&A 고도화 등)

**참고 자료:**

* LangServe 공식 문서: [https://python.langchain.com/docs/langserve/](https://python.langchain.com/docs/langserve/)
* FastAPI 공식 문서: [https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/)