## LangChain 블로그 시리즈 [6편]: 나만의 데이터와 대화하기 (2) - 임베딩과 벡터 스토어

안녕하세요! LangChain 블로그 시리즈 6편입니다. 지난 5편에서는 RAG(Retrieval-Augmented Generation)의 첫 단추로, 외부 문서를 LangChain으로 가져오고(`Document Loading`) LLM이 처리하기 좋은 크기로 나누는(`Text Splitting`) 방법을 배웠습니다. 그 결과, 우리는 이제 문서의 내용을 담고 있는 여러 개의 작은 텍스트 '조각(Chunk)'들을 가지게 되었습니다.

하지만 여기서 질문이 생깁니다. 사용자가 질문을 했을 때, 이 수많은 텍스트 조각 중에서 **어떤 조각이 사용자의 질문과 가장 관련이 깊을까요?** 단순히 키워드 매칭만으로는 문맥과 의미를 파악하기 어렵습니다. 우리는 텍스트의 '의미' 자체를 비교할 방법이 필요합니다.

바로 이 문제를 해결하는 핵심 기술이 **임베딩(Embeddings)** 이며, 임베딩된 데이터를 효율적으로 저장하고 검색하는 시스템이 **벡터 스토어(Vector Stores)** 입니다. 이번 시간에는 이 두 가지 개념에 대해 깊이 알아보고, RAG 파이프라인의 '저장' 단계를 구축해 보겠습니다.

### 임베딩(Embeddings): 텍스트에 의미 좌표 부여하기

**임베딩**이란 텍스트(단어, 문장, 문단 등)를 의미론적 정보를 함축하는 **숫자 벡터(Numerical Vector)** 로 변환하는 과정을 말합니다. 마치 지도 위의 좌표처럼, 임베딩은 텍스트의 의미를 다차원 공간상의 한 점으로 표현합니다. 이 공간에서는 **의미가 비슷한 텍스트일수록 서로 가까운 위치**에 놓이게 됩니다.

예를 들어, "강아지"와 "개"라는 단어는 임베딩 공간에서 매우 가까운 벡터로 표현될 것이고, "자동차"는 이들과 멀리 떨어진 벡터로 표현될 것입니다. 문장 수준에서도 마찬가지입니다. "오늘 날씨가 좋다"와 "화창한 날이다"는 유사한 의미를 가지므로 가까운 벡터로, "파이썬 코드를 짜고 있다"는 전혀 다른 의미이므로 먼 벡터로 변환됩니다.

이러한 변환 작업은 **임베딩 모델(Embedding Model)** 이 수행합니다. 임베딩 모델은 대규모 텍스트 데이터셋으로 사전 학습되어, 텍스트 간의 복잡한 의미 관계를 파악하고 이를 벡터로 표현하는 능력을 갖추고 있습니다.

LangChain에서는 다양한 임베딩 모델을 쉽게 사용할 수 있는 인터페이스를 제공합니다. 대표적인 예시는 다음과 같습니다.

**1. OpenAI 임베딩 (`OpenAIEmbeddings`)**

OpenAI에서 제공하는 강력한 임베딩 모델입니다. 높은 성능을 보여주지만, API 호출 비용이 발생하며 API 키 설정이 필요합니다.

```python
# 필요한 라이브러리 설치
# pip install langchain-openai

import os
from langchain_openai import OpenAIEmbeddings

# 환경 변수에서 OpenAI API 키 설정 (미리 설정 필요)
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"

# OpenAI 임베딩 모델 초기화
embeddings_model = OpenAIEmbeddings() # model="text-embedding-ada-002" 등 지정 가능

# 텍스트 임베딩 예시
text = "안녕하세요, LangChain 임베딩 테스트입니다."
embedded_text = embeddings_model.embed_query(text)
print(f"'{text}'의 임베딩 벡터 (일부):", embedded_text[:5]) # 매우 긴 벡터

# 여러 문서 임베딩 예시 (Document 객체 리스트 필요)
# documents = [...] # loader.load() 또는 splitter.split_documents() 결과
# embedded_docs = embeddings_model.embed_documents([doc.page_content for doc in documents])
# print(f"\n{len(documents)}개 문서 임베딩 완료. 첫 번째 문서 벡터 (일부):", embedded_docs[0][:5])
```

**2. Hugging Face 임베딩 (`HuggingFaceEmbeddings`)**

Hugging Face Hub에 공개된 다양한 오픈소스 임베딩 모델을 사용할 수 있게 해줍니다. 대표적으로 `Sentence Transformers` 라이브러리를 활용하며, 로컬 환경에서 무료로 실행할 수 있다는 장점이 있습니다. 한국어 성능이 좋은 모델(예: `jhgan/ko-sroberta-multitask`)을 선택할 수도 있습니다.

```python
# 필요한 라이브러리 설치
# pip install langchain-community sentence-transformers

from langchain_community.embeddings import HuggingFaceEmbeddings

# Hugging Face 임베딩 모델 초기화
# (처음 실행 시 모델 다운로드 시간이 소요될 수 있음)
model_name = "sentence-transformers/all-MiniLM-L6-v2" # 영어권에서 성능 좋은 경량 모델
# model_name = "jhgan/ko-sroberta-multitask" # 한국어 모델 예시
model_kwargs = {'device': 'cpu'} # CPU 사용 명시 (GPU 사용 가능 시 'cuda')
encode_kwargs = {'normalize_embeddings': False} # 정규화 여부

embeddings_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# 텍스트 임베딩 예시
text = "안녕하세요, LangChain 임베딩 테스트입니다."
embedded_text = embeddings_model.embed_query(text)
print(f"'{text}'의 임베딩 벡터 (일부):", embedded_text[:5])

# 여러 문서 임베딩 예시
# documents = [...]
# embedded_docs = embeddings_model.embed_documents([doc.page_content for doc in documents])
# print(f"\n{len(documents)}개 문서 임베딩 완료. 첫 번째 문서 벡터 (일부):", embedded_docs[0][:5])
```

이제 텍스트를 의미를 담은 벡터로 변환하는 방법을 알았습니다. 다음은 이 벡터들을 효율적으로 저장하고 검색할 차례입니다.

### 벡터 스토어(Vector Stores): 의미 기반 검색을 위한 특수 데이터베이스

텍스트 조각들을 임베딩하여 벡터로 만들었다면, 이제 이 벡터들을 저장할 공간이 필요합니다. 단순히 리스트나 일반 데이터베이스에 저장할 수도 있지만, 문제는 **검색 효율성**입니다. 사용자의 질문이 들어왔을 때, 그 질문을 임베딩한 벡터와 가장 유사한(가까운) 벡터들을 수많은 저장된 벡터들 중에서 빠르게 찾아내야 합니다.

이것이 **벡터 스토어(Vector Store)** 또는 **벡터 데이터베이스(Vector Database)** 가 필요한 이유입니다. 벡터 스토어는 고차원 벡터 데이터를 저장하고, 특정 벡터와 유사한 벡터들을 효율적으로 검색(주로 **유사도 검색 Similarity Search**)하는 데 최적화된 데이터베이스입니다. 내부적으로는 ANN(Approximate Nearest Neighbor) 검색 같은 알고리즘을 사용하여 빠른 속도로 유사 벡터를 찾습니다.

LangChain은 다양한 벡터 스토어와의 연동을 지원합니다. 크게 로컬 환경에서 실행되는 것과 클라우드 기반 서비스로 나눌 수 있습니다.

**1. 로컬/인메모리 벡터 스토어**

개발 환경이나 소규모 데이터셋에 적합하며, 설정이 비교적 간편합니다.

* **FAISS (`FAISS`)**: Facebook AI Research에서 개발한 벡터 유사도 검색 라이브러리입니다. 매우 빠르고 효율적이며, 주로 인메모리 방식으로 사용되지만 디스크에 저장하고 로드할 수도 있습니다. (`faiss-cpu` 또는 `faiss-gpu` 설치 필요)
* **Chroma (`Chroma`)**: 오픈소스 벡터 데이터베이스로, 기본적으로 디스크에 데이터를 저장하여 영속성을 가집니다. 로컬에서 실행하거나 서버 모드로 운영할 수 있습니다. (`chromadb` 설치 필요)

**로컬 벡터 스토어 사용 예시 (Chroma)**

```python
# 필요한 라이브러리 설치
# pip install chromadb

from langchain_community.vectorstores import Chroma
# from langchain_community.vectorstores import FAISS # FAISS 사용 시

# --- 이전 단계에서 준비된 가정 ---
# documents = [...] # splitter.split_documents() 로 얻은 Document 객체 리스트
# embeddings_model = HuggingFaceEmbeddings(...) # 또는 OpenAIEmbeddings() 등
# ---------------------------------

# Chroma 벡터 스토어 생성 및 데이터 저장
# Document 객체 리스트와 임베딩 모델을 전달하여 생성
# persist_directory를 지정하면 해당 경로에 데이터 저장
vector_store = Chroma.from_documents(
    documents=split_docs, # 분할된 Document 객체 리스트
    embedding=embeddings_model, # 사용할 임베딩 모델
    persist_directory="./chroma_db" # 저장될 디렉토리
)

# # FAISS 벡터 스토어 생성 예시 (인메모리)
# vector_store_faiss = FAISS.from_documents(split_docs, embeddings_model)
# # FAISS 로컬 저장/로드
# # vector_store_faiss.save_local("faiss_index")
# # loaded_vector_store = FAISS.load_local("faiss_index", embeddings_model)

print("벡터 스토어 생성이 완료되었습니다.")

# 저장된 벡터 스토어 로드 (애플리케이션 재시작 시 유용)
# loaded_vector_store = Chroma(persist_directory="./chroma_db", embedding_function=embeddings_model)
```

**2. 클라우드 기반 벡터 스토어**

대규모 데이터셋, 고가용성, 관리 편의성이 필요할 때 적합합니다.

* **Pinecone**: 대표적인 관리형 벡터 데이터베이스 서비스입니다. 확장성이 뛰어나고 다양한 기능을 제공합니다. (별도 가입 및 API 키 필요)
* **Weaviate**: 오픈소스 벡터 데이터베이스로, 클라우드 서비스 또는 자체 호스팅이 가능합니다.
* 이 외에도 다양한 클라우드 DB (Supabase pgvector, Redis 등)가 벡터 검색 기능을 지원하며 LangChain과 연동됩니다.

**로컬 vs 클라우드:**

* **로컬:** 설정 간편, 비용 무료, 빠른 프로토타이핑. 데이터 규모가 커지면 성능/관리 한계.
* **클라우드:** 뛰어난 확장성, 고가용성, 관리 용이. 서비스 비용 발생, 초기 설정 복잡성.

프로젝트의 규모와 요구사항에 맞는 벡터 스토어를 선택하는 것이 중요합니다.

### 파이프라인 구축: Load -> Split -> Embed -> Store

이제 5편과 6편에서 배운 내용을 합쳐 RAG의 데이터 준비 및 저장 파이프라인을 완성해 봅시다.

```python
# 필요한 라이브러리 임포트
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# --- 1. 문서 로드 (Load) ---
file_path = "./data/example.pdf" # 예시 PDF 파일 경로
loader = PyPDFLoader(file_path)
documents = loader.load()
print(f"'{file_path}'에서 {len(documents)} 페이지를 로드했습니다.")

# --- 2. 문서 분할 (Split) ---
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = text_splitter.split_documents(documents)
print(f"{len(documents)}개 페이지를 {len(split_docs)}개의 청크로 분할했습니다.")

# --- 3. 임베딩 모델 준비 (Embed) ---
model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}
embeddings_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
print(f"임베딩 모델({model_name})을 준비했습니다.")

# --- 4. 벡터 스토어에 저장 (Store) ---
persist_directory = "./chroma_db_example"
vector_store = Chroma.from_documents(
    documents=split_docs,
    embedding=embeddings_model,
    persist_directory=persist_directory
)
print(f"분할된 문서를 임베딩하여 '{persist_directory}'에 벡터 스토어로 저장했습니다.")

# (선택 사항) 벡터 스토어 강제 저장
vector_store.persist()
```

이 코드를 실행하면 `example.pdf` 파일의 내용이 로드되고, 작은 청크로 분할된 후, 각 청크가 임베딩 모델을 통해 벡터로 변환되어 `chroma_db_example` 디렉토리에 벡터 스토어로 저장됩니다. 이제 이 벡터 스토어를 사용하여 의미 기반 검색을 수행할 준비가 되었습니다.

### 저장된 데이터 검색하기: 유사도 검색 (Similarity Search)

벡터 스토어의 가장 중요한 기능은 사용자의 질문(쿼리)과 의미적으로 가장 유사한 텍스트 조각(문서 청크)을 찾아주는 것입니다. `similarity_search` 메서드를 사용하면 됩니다.

```python
# 사용자 질문
query = "What is Retrieval-Augmented Generation?"

# 벡터 스토어 로드 (이미 생성 및 저장된 경우)
# loaded_vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings_model)

# 유사도 검색 수행
# k: 반환할 결과(청크)의 수
similar_chunks = vector_store.similarity_search(query, k=3)

print(f"\n--- Query: '{query}' 와 유사한 상위 {len(similar_chunks)}개 청크 ---")
for i, chunk in enumerate(similar_chunks):
    print(f"--- Chunk {i+1} (Score: N/A for basic search) ---") # 일부 벡터스토어는 score 반환 가능
    print("Content:", chunk.page_content[:200]) # 내용 미리보기
    print("Metadata:", chunk.metadata)
    print("-" * 20)
```

`similarity_search`는 쿼리 텍스트를 내부적으로 임베딩 모델을 사용해 벡터로 변환한 다음, 벡터 스토어에 저장된 모든 벡터들과의 유사도(보통 코사인 유사도)를 계산하여 가장 높은 점수를 받은 `k`개의 벡터에 해당하는 원본 문서 청크(`Document` 객체)를 반환합니다.

### 정리 및 다음 단계

이번 6편에서는 RAG의 핵심 요소인 **임베딩**과 **벡터 스토어**에 대해 배웠습니다.

* **임베딩:** 텍스트를 의미를 담은 숫자 벡터로 변환하여, 의미적 유사성을 비교할 수 있게 합니다. (OpenAI, HuggingFace 모델 사용법)
* **벡터 스토어:** 임베딩된 벡터들을 효율적으로 저장하고, 쿼리 벡터와 유사한 벡터들을 빠르게 검색하는 데 최적화된 데이터베이스입니다. (Chroma, FAISS 등 로컬 옵션 소개)
* **파이프라인:** 문서 로드 -> 분할 -> 임베딩 -> 벡터 스토어 저장까지의 과정을 코드로 구현했습니다.
* **유사도 검색:** 저장된 벡터 스토어에서 사용자의 질문과 관련된 문서 청크를 찾는 방법을 확인했습니다.

이제 우리는 사용자의 질문과 관련된 정보를 우리의 데이터 속에서 찾아낼 수 있는 강력한 메커니즘을 갖추게 되었습니다!

다음 **[7편]** 에서는 드디어 이 모든 조각들을 하나로 합칠 차례입니다. **Retriever** 컴포넌트를 사용하여 벡터 스토어에서 관련 문서를 검색하고, 이 정보를 LLM의 프롬프트에 통합하여 최종 답변을 생성하는 **완전한 RAG Chain**을 구축하는 방법을 알아보겠습니다. 실제 질문-답변 챗봇의 모습을 갖추게 될 것입니다. 기대해주세요!

궁금한 점이나 어려운 부분은 댓글로 남겨주세요. 직접 코드를 실행하며 임베딩 모델과 벡터 스토어를 바꿔보는 실험도 큰 도움이 될 것입니다.