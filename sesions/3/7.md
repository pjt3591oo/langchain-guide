## LangChain 블로그 시리즈 [7편]: 나만의 데이터와 대화하기 (3) - Retriever와 RAG Chain 구축

안녕하세요! LangChain 시리즈 7편에 오신 것을 환영합니다. 지난 5편과 6편을 통해 우리는 외부 문서를 로드하고, 의미 있는 청크로 분할하고, 이 청크들을 임베딩하여 벡터 스토어에 저장하는 방법을 배웠습니다. 또한, 벡터 스토어의 `similarity_search` 기능을 이용해 사용자의 질문과 가장 관련성 높은 문서 청크들을 찾아낼 수 있게 되었습니다.

이제 RAG(Retrieval-Augmented Generation) 퍼즐의 마지막 조각을 맞출 시간입니다! 관련성 높은 정보(문서 청크)는 찾았는데, 이걸 어떻게 활용해서 LLM이 사용자의 질문에 **실제로 답변**하게 만들 수 있을까요?

이번 시간에는 LangChain의 **Retriever** 인터페이스를 사용하여 관련 문서를 효율적으로 가져오고, 이 정보를 LLM, 프롬프트 템플릿과 결합하여 완전한 **RAG Chain**을 구축하는 방법을 LangChain Expression Language(LCEL)를 통해 알아보겠습니다. 드디어 우리의 데이터에 기반하여 답변하는 간단한 Q&A 챗봇을 만들어 볼 것입니다!

### Retriever: 문서 검색 전문가

6편에서 우리는 벡터 스토어 객체의 `similarity_search` 메서드를 직접 호출하여 관련 문서를 검색했습니다. 하지만 LangChain은 문서 검색 로직을 추상화하고 표준화된 인터페이스를 제공하는 **Retriever** 컴포넌트를 제공합니다.

Retriever는 단순히 벡터 스토어의 유사도 검색을 감싸는 것 이상으로, 다양한 검색 전략을 적용하거나 여러 소스로부터 문서를 가져오는 복잡한 로직을 구현할 수 있는 확장성 있는 기반을 제공합니다. RAG Chain을 구축할 때는 일반적으로 벡터 스토어를 직접 사용하기보다 Retriever를 사용하는 것이 권장됩니다.

가장 기본적인 형태는 벡터 스토어 객체로부터 Retriever를 생성하는 것입니다.

```python
# 필요한 라이브러리 설치 (이전 단계 포함)
# pip install langchain langchain-openai langchain-community pypdf sentence-transformers chromadb faiss-cpu

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# --- 6편에서 생성 및 저장한 벡터 스토어 로드 가정 ---
persist_directory = "./chroma_db_example"
embeddings_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': False}
)
vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings_model)
# ----------------------------------------------------

# 벡터 스토어에서 Retriever 생성
retriever = vector_store.as_retriever()

print("Retriever가 생성되었습니다.")

# Retriever 직접 사용해보기 (결과는 Document 객체 리스트)
query = "What is Retrieval-Augmented Generation?"
retrieved_docs = retriever.invoke(query)

print(f"\n--- Retriever 결과 (Query: '{query}') ---")
for i, doc in enumerate(retrieved_docs):
    print(f"--- Document {i+1} ---")
    print(doc.page_content[:100]) # 내용 미리보기
    print(doc.metadata)
    print("-" * 15)
```

`as_retriever()`를 호출할 때 검색 옵션을 지정하여 Retriever의 동작을 상세하게 제어할 수 있습니다.

* **`search_type`**: 검색 유형을 지정합니다.
    * `"similarity"` (기본값): 쿼리와 가장 유사한 벡터를 가진 문서를 반환합니다.
    * `"mmr"` (Maximal Marginal Relevance): 유사도와 다양성을 함께 고려합니다. 즉, 쿼리와 유사하면서도 서로 중복되지 않는 다양한 정보를 가진 문서를 반환하려고 시도합니다. 정보가 중복되는 청크가 많은 경우 유용할 수 있습니다.
* **`search_kwargs`**: 검색 관련 추가 인자를 딕셔너리 형태로 전달합니다.
    * `{'k': 5}`: 반환할 문서(청크)의 개수를 지정합니다. 기본값은 보통 4개입니다.

```python
# MMR 검색을 사용하고 상위 3개 문서를 가져오는 Retriever 생성
retriever_mmr_k3 = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={'k': 3}
)

retrieved_docs_mmr = retriever_mmr_k3.invoke(query)
print(f"\n--- MMR Retriever 결과 (k=3) ---")
# ... 결과 출력 (위와 유사) ...
```

이제 우리는 사용자의 질문에 맞는 관련 문서를 가져오는 `retriever` 객체를 준비했습니다. 다음은 이 정보를 LLM에게 전달할 프롬프트를 만들 차례입니다.

### RAG를 위한 프롬프트 템플릿 작성

RAG의 핵심은 LLM이 **제공된 문맥(Context) 정보에 기반하여** 답변하도록 유도하는 것입니다. 따라서 프롬프트 템플릿에는 LLM에게 이 지침을 명확하게 전달하는 내용이 포함되어야 합니다. 또한, Retriever를 통해 가져온 문서 내용(`context`)과 사용자의 원본 질문(`question`)을 삽입할 위치를 지정해야 합니다.

LangChain의 `ChatPromptTemplate`을 사용하여 이를 구현할 수 있습니다.

```python
from langchain_core.prompts import ChatPromptTemplate

# RAG 프롬프트 템플릿 정의
template = """
You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use three sentences maximum and keep the answer concise.

Context:
{context}

Question:
{question}

Answer:
"""

prompt = ChatPromptTemplate.from_template(template)

print("RAG 프롬프트 템플릿이 준비되었습니다.")
# print(prompt.format(context="여기에 검색된 문서 내용이 들어갑니다.", question="사용자의 질문입니다."))
```

이 템플릿은 LLM에게 역할을 부여하고, 검색된 `context`를 기반으로 `question`에 답하라고 지시합니다. 모르는 경우 모른다고 답하라는 지침은 LLM의 환각(Hallucination)을 줄이는 데 도움이 됩니다. `{context}`와 `{question}`은 나중에 LCEL 체인에서 동적으로 채워질 변수입니다.

### LCEL로 RAG Chain 구축하기

이제 모든 준비가 끝났습니다! Retriever, 프롬프트 템플릿, 그리고 LLM(예: `ChatOpenAI`)을 LangChain Expression Language(LCEL)의 파이프(`|`) 연산자를 사용하여 하나의 Chain으로 엮어 보겠습니다.

RAG Chain의 일반적인 데이터 흐름은 다음과 같습니다.

1.  **입력**: 사용자의 질문 (문자열)
2.  **문서 검색 (Retrieve)**: Retriever가 질문을 받아 관련 문서(`context`)를 검색합니다.
3.  **프롬프트 포맷팅 (Format Prompt)**: 검색된 `context`와 원본 `question`을 프롬프트 템플릿에 삽입합니다.
4.  **LLM 호출 (Generate)**: 포맷팅된 프롬프트를 LLM에 전달하여 답변을 생성합니다.
5.  **출력 파싱 (Parse Output)**: LLM의 응답(주로 `ChatMessage` 객체)에서 최종 답변 문자열만 추출합니다.

LCEL을 사용하면 이 흐름을 매우 직관적으로 표현할 수 있습니다.

```python
import os
from langchain_openai import ChatOpenAI
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# --- 환경 변수 설정 (OpenAI API 키) ---
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"
# --------------------------------------

# --- 이전 단계에서 준비된 객체들 ---
# retriever = vector_store.as_retriever(...)
# prompt = ChatPromptTemplate.from_template(template)
# --------------------------------------

# 1. LLM 준비
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0) # 또는 다른 Chat 모델

# 2. 출력 파서 준비 (LLM 응답에서 문자열만 추출)
output_parser = StrOutputParser()

# 3. 검색된 Document 리스트를 단일 문자열로 포맷하는 함수
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# 4. RAG Chain 구성 (LCEL 사용)

# RunnableParallel을 사용하여 retriever와 question을 병렬로 처리하고 결과를 딕셔너리로 묶음
# 'context' 키에는 retriever 결과가, 'question' 키에는 원본 질문이 담김
setup_and_retrieval = RunnableParallel(
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
)

# 위에서 준비된 딕셔너리를 prompt, llm, output_parser로 순차적으로 연결
rag_chain = setup_and_retrieval | prompt | llm | output_parser

print("RAG Chain이 성공적으로 구성되었습니다!")

# rag_chain의 흐름 시각화 (Jupyter Notebook 등 환경에서)
# from langchain.globals import set_debug
# set_debug(True)
# rag_chain.get_graph().print_ascii()
# set_debug(False)
```

코드를 자세히 살펴보겠습니다.

* `RunnableParallel`: 여러 Runnable(여기서는 `retriever | format_docs`와 `RunnablePassthrough`)을 동시에 실행하고, 그 결과를 지정된 키(`context`, `question`)를 가진 딕셔너리로 만듭니다.
    * `retriever | format_docs`: Retriever를 실행하고(`invoke`), 그 결과(`Document` 리스트)를 `format_docs` 함수에 넘겨 하나의 긴 문자열로 만듭니다. 이 문자열이 `context` 키의 값이 됩니다.
    * `RunnablePassthrough()`: 입력을 변경 없이 그대로 다음 단계로 전달합니다. 여기서는 사용자의 원본 질문 문자열이 `question` 키의 값이 됩니다.
* `setup_and_retrieval | prompt | llm | output_parser`: `RunnableParallel`의 출력(context와 question이 담긴 딕셔너리)을 받아 `prompt.format()`에 필요한 인자로 사용하고, 포맷팅된 프롬프트를 `llm`에 전달하고, 마지막으로 `llm`의 출력을 `output_parser`를 통해 최종 문자열 답변으로 변환합니다.

### RAG Chain 실행: 나만의 데이터에게 질문하기!

이제 완성된 `rag_chain`을 사용하여 우리의 문서에 대해 질문해 봅시다.

```python
# RAG Chain 실행
question = "What are the main components of LangChain?"
# question = "How does MMR search work?" # 문서 내용에 따라 질문 변경 가능

print(f"\n--- 질문: {question} ---")

# Chain 실행 (invoke 사용)
answer = rag_chain.invoke(question)

print("\n--- 답변 ---")
print(answer)

# (선택) 스트리밍 방식으로 답변 받기 (지원하는 LLM 및 설정 필요)
# print("\n--- 스트리밍 답변 ---")
# for chunk in rag_chain.stream(question):
#     print(chunk, end="", flush=True)
# print()
```

`invoke()` 메서드를 호출하면 LCEL 체인이 실행되어 질문에 대한 답변이 생성됩니다. 이 답변은 LLM이 임의로 생성한 것이 아니라, 우리가 제공한 문서(PDF 파일 등)에서 검색된 관련 내용을 바탕으로 생성되었을 것입니다. 만약 관련 내용이 없다면, 프롬프트의 지침에 따라 "모른다"고 답할 수도 있습니다.

(참고) 만약 LangChain 디버그 모드를 활성화하면 (`set_debug(True)`) 체인이 실행될 때 각 단계의 입력과 출력을 상세하게 확인할 수 있어, RAG 프로세스를 이해하고 디버깅하는 데 큰 도움이 됩니다.

### 정리 및 다음 단계

축하합니다! 이번 7편에서는 RAG 파이프라인의 모든 구성 요소를 통합하여 실제 작동하는 질문-답변 시스템을 구축했습니다.

* **Retriever:** 벡터 스토어로부터 관련 문서를 검색하는 표준 인터페이스를 사용하고 설정하는 방법을 배웠습니다.
* **RAG Prompt:** LLM이 검색된 문맥을 기반으로 답변하도록 유도하는 프롬프트 템플릿을 작성했습니다.
* **LCEL Chain:** Retriever, 프롬프트, LLM, 출력 파서를 LCEL을 사용하여 하나의 강력하고 유연한 RAG Chain으로 결합했습니다.
* **Q&A:** 구축된 RAG Chain을 실행하여 우리만의 데이터에 대해 질문하고 답변을 얻는 것을 확인했습니다.

이제 여러분은 LangChain을 사용하여 특정 문서나 데이터베이스에 기반한 정보성 답변을 생성하는 애플리케이션의 핵심 로직을 구현할 수 있게 되었습니다!

다음 **[8편]** 에서는 챗봇에 '기억력'을 부여하는 **Memory** 컴포넌트에 대해 알아봅니다. 지금까지 만든 Q&A 시스템은 이전 대화 내용을 기억하지 못하지만, Memory를 추가하면 사용자와의 대화 맥락을 유지하며 더 자연스러운 상호작용이 가능해집니다. 기대해주세요!

직접 다른 문서를 로드해보고, 다양한 질문을 던져보며 RAG Chain의 성능을 테스트해보세요. 궁금한 점은 언제든지 댓글로 남겨주시기 바랍니다.