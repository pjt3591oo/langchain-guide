
## LangChain 블로그 시리즈 [5편]: 나만의 데이터와 대화하기 (1) - 문서 로드 및 분할

안녕하세요\! LangChain 블로그 시리즈에 오신 것을 환영합니다. 지난 시간에는 LangChain의 핵심 요소인 Model, Prompt, Output Parser와 이를 유기적으로 연결하는 Chain, 그리고 강력한 LCEL(LangChain Expression Language)에 대해 알아보았습니다. 이를 통해 LLM을 단순히 호출하는 것을 넘어, 정해진 워크플로우를 따라 작동하는 간단한 애플리케이션을 만들 수 있게 되었습니다.

하지만 LLM의 지식은 특정 시점까지의 데이터로 학습되었고, 세상의 모든 정보나 여러분의 개인적인 문서를 알고 있지는 못합니다. 만약 LLM이 최신 정보를 참조하거나, 회사의 내부 문서, 개인 노트 등을 기반으로 답변하게 할 수 있다면 어떨까요?

이번 시간부터는 바로 이 질문에 답하는 **Retrieval-Augmented Generation (RAG)** 에 대해 알아봅니다. RAG는 외부 데이터 소스로부터 관련 정보를 검색(Retrieve)하고, 이 정보를 LLM에게 제공하여 답변 생성을 보강(Augment)하는 강력한 기법입니다.

이번 5편에서는 RAG 파이프라인의 첫 단계인 **"외부 문서를 LangChain으로 가져오고(Loading), LLM이 처리하기 좋은 크기로 나누는(Splitting)"** 과정에 대해 자세히 살펴보겠습니다.

### RAG란 무엇이고 왜 필요할까요?

LLM(Large Language Model)은 놀라운 능력을 가지고 있지만, 몇 가지 본질적인 한계점을 지닙니다.

1.  **지식의 한계 (Knowledge Cutoff):** 모델이 학습된 데이터는 특정 시점까지의 정보입니다. 그 이후의 최신 정보나 사건에 대해서는 알지 못합니다.
2.  **환각 (Hallucination):** LLM은 때때로 사실이 아닌 정보를 그럴듯하게 생성하는 경향이 있습니다.
3.  **접근 불가 정보 (Private Data):** LLM은 인터넷에 공개되지 않은 여러분의 개인 파일, 회사 내부 문서, 데이터베이스 내용 등은 알 수 없습니다.

**RAG**는 이러한 한계를 극복하기 위한 효과적인 접근 방식입니다. RAG의 핵심 아이디어는 다음과 같습니다.

1.  **검색 (Retrieve):** 사용자의 질문과 관련된 정보를 외부 데이터 소스(문서, 데이터베이스 등)에서 찾습니다.
2.  **보강 (Augment):** 검색된 관련 정보를 사용자의 질문과 함께 LLM의 프롬프트에 포함시킵니다.
3.  **생성 (Generate):** LLM은 주어진 질문과 검색된 문맥 정보를 바탕으로 답변을 생성합니다.

마치 우리가 시험을 볼 때 참고 자료(오픈북)를 활용하는 것과 비슷합니다. LLM이 필요한 정보를 "참고"할 수 있게 해줌으로써, RAG는 더 정확하고, 최신 정보를 반영하며, 특정 도메인 지식이나 개인 데이터에 기반한 답변 생성을 가능하게 합니다.

이제 RAG를 구현하기 위한 첫걸음, 데이터를 LangChain으로 가져오는 방법을 알아봅시다.

### 1단계: 문서 로딩 (Document Loading) - 다양한 형식의 데이터를 LangChain으로\!

RAG의 첫 번째 단계는 LLM이 참조할 문서를 LangChain 애플리케이션으로 가져오는 것입니다. 다행히 LangChain은 다양한 형식의 문서를 로드할 수 있는 **Document Loader** 컴포넌트를 풍부하게 제공합니다.

Document Loader는 파일 경로, 웹 URL 등 다양한 소스로부터 데이터를 읽어와 표준화된 `Document` 객체 형식으로 변환해줍니다. 각 `Document` 객체는 주로 두 가지 정보를 담고 있습니다.

  * `page_content` (문자열): 문서의 실제 텍스트 내용
  * `metadata` (딕셔너리): 문서의 출처(파일 경로, URL 등), 페이지 번호 등 부가 정보

몇 가지 자주 사용되는 Document Loader를 살펴보겠습니다.

**1. 텍스트 파일 로더 (`TextLoader`)**

가장 기본적인 로더로, `.txt` 파일을 로드합니다.

```python
# 필요한 라이브러리 설치
# pip install langchain

from langchain_community.document_loaders import TextLoader

# 텍스트 파일 로더 생성 (파일 경로 지정)
loader = TextLoader("./data/my_document.txt", encoding='utf-8') # 인코딩 주의

# 문서 로드
documents = loader.load()

# 로드된 문서 확인 (리스트 형태)
print(f"Loaded {len(documents)} document(s).")
if documents:
    print("--- First Document ---")
    print("Content Preview:", documents[0].page_content[:100]) # 내용 미리보기
    print("Metadata:", documents[0].metadata)
```

**2. PDF 로더 (`PyPDFLoader`)**

PDF 파일을 로드합니다. 내부적으로 `pypdf` 라이브러리를 사용하므로 설치가 필요합니다.

```python
# 필요한 라이브러리 설치
# pip install pypdf langchain_community

from langchain_community.document_loaders import PyPDFLoader

# PDF 파일 로더 생성
loader = PyPDFLoader("./data/example.pdf")

# 문서 로드 (PDF 페이지별로 Document 객체가 생성될 수 있음)
pages = loader.load_and_split() # load() 또는 load_and_split() 사용 가능

print(f"Loaded {len(pages)} page(s) from PDF.")
if pages:
    print("--- First Page Document ---")
    print("Content Preview:", pages[0].page_content[:100])
    print("Metadata:", pages[0].metadata) # 페이지 번호 등이 메타데이터에 포함됨
```

**3. 웹 기반 로더 (`WebBaseLoader`)**

웹 페이지의 HTML 내용을 가져와 텍스트로 변환합니다. `bs4` 라이브러리가 필요합니다.

```python
# 필요한 라이브러리 설치
# pip install bs4 langchain_community

from langchain_community.document_loaders import WebBaseLoader

# 웹 페이지 로더 생성 (URL 리스트 전달 가능)
loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")

# 문서 로드
documents = loader.load()

print(f"Loaded {len(documents)} document(s) from web.")
if documents:
    print("--- Web Document ---")
    print("Content Preview:", documents[0].page_content[:200])
    print("Metadata:", documents[0].metadata) # URL이 메타데이터에 포함됨
```

이 외에도 LangChain은 CSV, JSON, HTML, Notion, Google Drive, Slack 등 수많은 데이터 소스를 위한 로더를 제공합니다. 필요한 로더는 [LangChain 공식 문서 (Document Loaders)](https://www.google.com/search?q=https://python.langchain.com/docs/modules/data_connection/document_loaders/)에서 찾아볼 수 있습니다.

이제 문서를 성공적으로 LangChain의 `Document` 객체로 가져왔습니다. 하지만 이 문서 덩어리들을 그대로 LLM에게 전달하기에는 너무 클 수 있습니다. 다음 단계로 넘어가 봅시다.

### 2단계: 텍스트 분할 (Text Splitting) - 문서를 의미 있는 조각으로 나누기

LLM은 한 번에 처리할 수 있는 입력 텍스트의 양에 제한이 있습니다. 이를 **컨텍스트 창(Context Window)** 이라고 하며, 모델마다 다릅니다 (예: gpt-3.5-turbo는 4k 또는 16k 토큰, gpt-4는 8k, 32k 또는 128k 토큰).

로드한 문서 전체가 이 컨텍스트 창 크기를 넘는 경우가 많습니다. 또한, 관련 정보를 효율적으로 검색(Retrieve)하기 위해서도 문서를 더 작은 단위로 나누는 것이 유리합니다. 너무 큰 덩어리보다는, 특정 주제나 내용을 담고 있는 적절한 크기의 "조각(Chunk)"으로 나누어야 검색 정확도를 높일 수 있습니다.

이때 사용되는 것이 **Text Splitter** 입니다. Text Splitter는 긴 `Document` 객체를 받아, 설정된 기준에 따라 여러 개의 작은 `Document` 객체(청크)로 분할합니다.

몇 가지 주요 Text Splitter 전략을 살펴봅시다.

**1. 문자 기반 분할 (`CharacterTextSplitter`)**

가장 간단한 방식으로, 지정된 문자 수(`chunk_size`)를 기준으로 텍스트를 나눕니다. 문맥 유지를 위해 청크 간에 겹치는 부분(`chunk_overlap`)을 설정할 수 있습니다.

```python
from langchain.text_splitter import CharacterTextSplitter

# 예시 텍스트 (실제로는 loader.load()로 얻은 documents 리스트 사용)
long_text = """
LangChain은 LLM을 활용한 애플리케이션 개발을 돕는 프레임워크입니다.
다양한 컴포넌트(Models, Prompts, Chains, Agents, Memory, Retrievers 등)를 제공하여
개발자가 복잡한 워크플로우를 쉽게 구축할 수 있도록 지원합니다.
특히 RAG(Retrieval-Augmented Generation)는 외부 데이터를 활용하여 LLM의 한계를 극복하는 강력한 기능입니다.
이번 시간에는 문서 로딩과 텍스트 분할에 대해 배우고 있습니다. 다음 시간에는 임베딩과 벡터 스토어를 다룰 예정입니다.
"""

# CharacterTextSplitter 생성
text_splitter = CharacterTextSplitter(
    separator="\n\n",  # 문단 기준으로 먼저 나누도록 시도
    chunk_size=100,    # 청크 크기 (문자 수)
    chunk_overlap=20,  # 청크 간 겹침 (문자 수)
    length_function=len, # 길이 계산 함수
)

# 텍스트 분할 (Document 객체 또는 문자열 리스트를 입력으로 받음)
# 실제 사용 시: chunks = text_splitter.split_documents(documents)
chunks = text_splitter.split_text(long_text)

print(f"Split into {len(chunks)} chunks.")
for i, chunk in enumerate(chunks):
    print(f"--- Chunk {i+1} ---")
    print(chunk)
    print("-" * 20)
```

  * `separator`: 텍스트를 우선적으로 나눌 구분자를 지정합니다. 기본값은 `"\n\n"` (문단)입니다.
  * `chunk_size`: 각 청크의 최대 크기를 지정합니다.
  * `chunk_overlap`: 인접한 청크 사이에 겹치게 할 문자 수를 지정합니다. 이는 청크 경계에서 문맥이 끊어지는 것을 방지하는 데 도움이 됩니다.

**2. 재귀적 문자 분할 (`RecursiveCharacterTextSplitter`)**

`CharacterTextSplitter`보다 좀 더 발전된 방식으로, 여러 구분자 리스트(`["\n\n", "\n", " ", ""]`가 기본값)를 순서대로 시도하며 텍스트를 분할합니다. 즉, 문단(`\n\n`)으로 먼저 나누려 하고, 너무 길면 줄바꿈(`\n`)으로, 그래도 길면 공백(`     `)으로, 최후에는 문자 단위로 나누어 `chunk_size`를 맞추려고 노력합니다. 이 방식은 문맥(단락, 문장 등)을 최대한 유지하려는 경향이 있어 일반적으로 권장됩니다.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# RecursiveCharacterTextSplitter 생성
recursive_splitter = RecursiveCharacterTextSplitter(
    # separators=["\n\n", "\n", " ", ""], # 기본값, 필요시 커스텀 가능
    chunk_size=100,
    chunk_overlap=20,
    length_function=len,
)

# 텍스트 분할
# 실제 사용 시: chunks = recursive_splitter.split_documents(documents)
recursive_chunks = recursive_splitter.split_text(long_text)

print(f"Split into {len(recursive_chunks)} chunks using Recursive splitter.")
for i, chunk in enumerate(recursive_chunks):
    print(f"--- Chunk {i+1} ---")
    print(chunk)
    print("-" * 20)
```

이 외에도 특정 토큰(예: `TiktokenTextSplitter`)을 기준으로 나누거나, 코드(`PythonCodeTextSplitter` 등), 마크다운(`MarkdownTextSplitter`), 의미론적 유사성(`SemanticChunker` - 고급)을 기반으로 분할하는 다양한 Text Splitter가 존재합니다.

**어떤 Splitter를 선택해야 할까요?**

  * **일반 텍스트:** `RecursiveCharacterTextSplitter`가 좋은 시작점입니다.
  * **코드:** 해당 언어에 맞는 Code Splitter를 사용하는 것이 좋습니다.
  * **성능/비용 중요:** LLM의 토큰 수를 정확히 계산하고 싶다면 `TiktokenTextSplitter`를 고려할 수 있습니다.
  * **고급:** 의미적으로 연관된 내용을 묶고 싶다면 `SemanticChunker` (임베딩 모델 필요)를 탐색해볼 수 있습니다.

중요한 것은 문서를 LLM의 컨텍스트 창 제약 조건 내에서 **의미 있는 단위**로 나누는 것입니다. `chunk_size`와 `chunk_overlap` 값을 조절하며 실험해보는 것이 좋습니다.

### 정리 및 다음 단계

이번 시간에는 RAG의 개념을 소개하고, 외부 데이터를 LangChain 애플리케이션으로 가져오는 첫 두 단계인 **Document Loading**과 **Text Splitting**에 대해 알아보았습니다.

  * **Document Loaders**를 사용하여 다양한 형식(txt, pdf, web 등)의 문서를 `Document` 객체로 로드했습니다.
  * **Text Splitters** (특히 `RecursiveCharacterTextSplitter`)를 사용하여 로드된 문서를 LLM이 처리하기 쉽고 검색에 용이한 작은 청크(chunk)들로 분할했습니다.

이제 우리는 외부 문서를 LangChain이 다룰 수 있는 형태의 '조각'들로 준비시켰습니다. 하지만 이 텍스트 조각들을 컴퓨터가 이해하고 서로 비교하려면 어떻게 해야 할까요? 바로 여기서 **임베딩(Embeddings)** 과 **벡터 스토어(Vector Stores)** 가 등장합니다.

다음 **[6편]** 에서는 분할된 텍스트 청크들을 벡터 공간에 표현하는 **임베딩**과, 이 벡터들을 효율적으로 저장하고 검색할 수 있는 **벡터 스토어**에 대해 자세히 알아보겠습니다. RAG의 핵심 구성 요소를 향해 한 걸음 더 나아가 봅시다\!

궁금한 점이 있다면 언제든지 댓글로 남겨주세요. 직접 코드를 실행해보며 다양한 로더와 스플리터를 사용해보시는 것을 추천합니다\!